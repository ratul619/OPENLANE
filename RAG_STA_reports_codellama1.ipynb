{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratul619/OPENLANE/blob/main/RAG_STA_reports_codellama1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0QCOTLkdZZ3",
        "outputId": "313ee2df-729a-48f6-fa5d-2b7ca3e87b1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.56.2\n",
            "Uninstalling transformers-4.56.2:\n",
            "  Successfully uninstalled transformers-4.56.2\n",
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: accelerate 1.10.1\n",
            "Uninstalling accelerate-1.10.1:\n",
            "  Successfully uninstalled accelerate-1.10.1\n",
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: sentence-transformers 5.1.1\n",
            "Uninstalling sentence-transformers-5.1.1:\n",
            "  Successfully uninstalled sentence-transformers-5.1.1\n",
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.1.0 (from versions: 2.2.0+cu118, 2.2.1+cu118, 2.2.2+cu118, 2.3.0+cu118, 2.3.1+cu118, 2.4.0+cu118, 2.4.1+cu118, 2.5.0+cu118, 2.5.1+cu118, 2.6.0+cu118, 2.7.0+cu118, 2.7.1+cu118)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.1.0\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting transformers==4.35.0\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (2.32.4)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.35.0)\n",
            "  Downloading tokenizers-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.0) (1.1.10)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.0) (2025.8.3)\n",
            "Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.35.3\n",
            "    Uninstalling huggingface-hub-0.35.3:\n",
            "      Successfully uninstalled huggingface-hub-0.35.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.17.1 requires accelerate>=0.21.0, which is not installed.\n",
            "peft 0.17.1 requires torch>=1.13.0, which is not installed.\n",
            "timm 1.0.20 requires torch, which is not installed.\n",
            "timm 1.0.20 requires torchvision, which is not installed.\n",
            "peft 0.17.1 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "datasets 4.0.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "gradio-client 1.13.3 requires huggingface-hub<2.0,>=0.19.3, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "gradio 5.47.2 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "diffusers 0.35.1 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.35.0\n",
            "Collecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (4.35.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (4.67.1)\n",
            "Collecting torch>=1.6.0 (from sentence-transformers==2.2.2)\n",
            "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision (from sentence-transformers==2.2.2)\n",
            "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (1.16.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (3.9.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (0.2.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers==2.2.2) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.19.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (25.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.27.3)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->sentence-transformers==2.2.2) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->sentence-transformers==2.2.2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.6.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->sentence-transformers==2.2.2) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.8.3)\n",
            "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m137.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m153.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=d8b641b05d27cd81a7f6ee029e24f6f077c4e55d57a75947c9e0893d2ffe42e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/3b/21/aa025e9c81a6cda4b8358756a756677b0969b4bc69be6dd5da\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, torchvision, sentence-transformers\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.17.1 requires accelerate>=0.21.0, which is not installed.\n",
            "peft 0.17.1 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 sentence-transformers-2.2.2 torch-2.8.0 torchvision-0.23.0\n",
            "Collecting accelerate==0.24.0\n",
            "  Downloading accelerate-0.24.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.24.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.24.0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.24.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==0.24.0) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==0.24.0) (2.8.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from accelerate==0.24.0) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate==0.24.0) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.24.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->accelerate==0.24.0) (4.67.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.24.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.24.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->accelerate==0.24.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->accelerate==0.24.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->accelerate==0.24.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->accelerate==0.24.0) (2025.8.3)\n",
            "Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.17.1 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.24.0\n",
            "Collecting bitsandbytes==0.41.0\n",
            "  Downloading bitsandbytes-0.41.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Downloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.41.0\n",
            "Collecting chromadb==0.4.15\n",
            "  Downloading chromadb-0.4.15-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (2.32.4)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (2.11.9)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.15)\n",
            "  Downloading chroma-hnswlib-0.7.3.tar.gz (31 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (0.118.0)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.15) (0.37.0)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.4.15)\n",
            "  Downloading posthog-6.7.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (4.15.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.15)\n",
            "  Downloading pulsar_client-3.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb==0.4.15)\n",
            "  Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==0.4.15)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (0.14.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb==0.4.15)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (1.75.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb==0.4.15)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (0.19.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb==0.4.15)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (8.5.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb==0.4.15) (2.0.2)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.95.2->chromadb==0.4.15) (0.48.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.9.0.post0)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (6.0.3)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.15) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb==0.4.15)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb==0.4.15)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.15)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.15) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.15) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.15) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.15) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.15) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb==0.4.15) (0.58b0)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.4.15)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog>=2.4.0->chromadb==0.4.15) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb==0.4.15) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb==0.4.15) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb==0.4.15) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28->chromadb==0.4.15) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.28->chromadb==0.4.15) (3.10)\n",
            "Requirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb==0.4.15) (0.17.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb==0.4.15) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb==0.4.15) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb==0.4.15) (13.9.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.15) (0.16.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.15)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.15) (1.1.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb==0.4.15)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.15)\n",
            "  Downloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.15) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.15) (3.19.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.15) (2025.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.15) (3.23.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.15) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.15) (2.19.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.15) (4.11.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.15)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb==0.4.15) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.15) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.15) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.15) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.15) (0.6.1)\n",
            "Downloading chromadb-0.4.15-py3-none-any.whl (479 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.8/479.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl (19 kB)\n",
            "Downloading posthog-6.7.6-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pulsar_client-3.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m132.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: chroma-hnswlib, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.3-cp312-cp312-linux_x86_64.whl size=2530625 sha256=75317ee1e20c2d53422e3104e957d57bfb78466eb61f53864025aa148980c26b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/14/b5/68c4f2e056600c0348a94efba92dc975686ab72b714e0ca3d6\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=08e8c3d53ace095c61d0797a7c1d4186dc9c81b941217e9a09263639ec84640f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built chroma-hnswlib pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pulsar-client, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, watchfiles, coloredlogs, posthog, onnxruntime, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "Successfully installed backoff-2.2.1 bcrypt-5.0.0 chroma-hnswlib-0.7.3 chromadb-0.4.15 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-34.1.0 onnxruntime-1.23.0 opentelemetry-exporter-otlp-proto-grpc-1.37.0 posthog-6.7.6 pulsar-client-3.8.0 pypika-0.48.9 urllib3-2.3.0 uvloop-0.21.0 watchfiles-1.1.0\n",
            "Requirement already satisfied: huggingface-hub==0.17.3 in /usr/local/lib/python3.12/dist-packages (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.17.3) (3.19.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.17.3) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.17.3) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.17.3) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.17.3) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.17.3) (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.17.3) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.17.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.17.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.17.3) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub==0.17.3) (2025.8.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.14.1)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement safetensor (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for safetensor\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Clean installation for Google Colab - Run this FIRST\n",
        "!pip uninstall -y transformers torch torchvision accelerate bitsandbytes sentence-transformers\n",
        "!pip cache purge\n",
        "\n",
        "# Install compatible versions in correct order\n",
        "!pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers==4.35.0\n",
        "!pip install sentence-transformers==2.2.2\n",
        "!pip install accelerate==0.24.0\n",
        "!pip install bitsandbytes==0.41.0\n",
        "!pip install chromadb==0.4.15\n",
        "!pip install huggingface-hub==0.17.3\n",
        "!pip install numpy scipy tokenizers safetensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCw1xGLZe10h",
        "outputId": "f319b9e0-dc82-42f5-d327-605feec77abd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ PyTorch: 2.8.0+cu128\n",
            "✅ CUDA available: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Transformers: 4.35.0\n",
            "✅ SentenceTransformers: Import successful\n",
            "✅ AutoTokenizer & AutoModelForCausalLM: Import successful\n",
            "✅ ChromaDB: Import successful\n",
            "\n",
            "🎉 All imports successful! Ready to run your code.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"✅ PyTorch: {torch.__version__}\")\n",
        "print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "import transformers\n",
        "print(f\"✅ Transformers: {transformers.__version__}\")\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "print(\"✅ SentenceTransformers: Import successful\")\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "print(\"✅ AutoTokenizer & AutoModelForCausalLM: Import successful\")\n",
        "\n",
        "import chromadb\n",
        "print(\"✅ ChromaDB: Import successful\")\n",
        "\n",
        "print(\"\\n🎉 All imports successful! Ready to run your code.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_PFSvOpfOul",
        "outputId": "119e59b0-86e2-4350-9830-a051a3bc809f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "\n",
        "print(\"✅ All imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "$pip install nbformat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgq5moAs08s8",
        "outputId": "c91521ec-8936-4c8a-c65a-e52e18dad305"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (5.10.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat) (4.25.1)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.8.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.12/dist-packages (from nbformat) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat) (0.27.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import json\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from typing import List, Dict, Any, Optional\n",
        "import shutil\n",
        "import logging\n",
        "\n",
        "# Suppress ChromaDB telemetry errors\n",
        "logging.getLogger(\"chromadb.telemetry.product.posthog\").setLevel(logging.ERROR)\n",
        "\n",
        "class ImprovedLocalTimingRAG:\n",
        "    def __init__(self, model_name: str = \"codellama/CodeLlama-7b-Instruct-hf\"):\n",
        "        self.model_name = model_name\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.llm_model, self.tokenizer = self._load_local_llm()\n",
        "\n",
        "        # Get device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Use in-memory database to avoid file permission issues\n",
        "        self._setup_database()\n",
        "\n",
        "        self.history = []\n",
        "        self.current_path_index = 0  # Track current path for \"next\" queries\n",
        "\n",
        "    def _setup_database(self):\n",
        "        \"\"\"Setup ChromaDB with in-memory storage\"\"\"\n",
        "        try:\n",
        "            # Use in-memory database to avoid file permission issues\n",
        "            self.client = chromadb.Client()\n",
        "            # Try to get existing collection, if it exists, delete it first\n",
        "            try:\n",
        "                existing_collection = self.client.get_collection(name=\"timing_reports\")\n",
        "                self.client.delete_collection(name=\"timing_reports\")\n",
        "            except:\n",
        "                pass  # Collection doesn't exist, which is fine\n",
        "\n",
        "            self.collection = self.client.create_collection(name=\"timing_reports\")\n",
        "            print(\"Using in-memory database\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up database: {e}\")\n",
        "            # Fallback to persistent client with a different path\n",
        "            try:\n",
        "                # Clear existing database\n",
        "                if os.path.exists(\"./temp_chroma_db\"):\n",
        "                    shutil.rmtree(\"./temp_chroma_db\")\n",
        "\n",
        "                self.client = chromadb.PersistentClient(path=\"./temp_chroma_db\")\n",
        "                self.collection = self.client.create_collection(name=\"timing_reports\")\n",
        "                print(\"Using persistent database at ./temp_chroma_db\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Error with persistent database: {e2}\")\n",
        "                # Last resort - create a new in-memory client\n",
        "                self.client = chromadb.Client()\n",
        "                try:\n",
        "                    existing_collection = self.client.get_collection(name=\"timing_reports\")\n",
        "                    self.client.delete_collection(name=\"timing_reports\")\n",
        "                except:\n",
        "                    pass\n",
        "                self.collection = self.client.create_collection(name=\"timing_reports\")\n",
        "                print(\"Using fallback in-memory database\")\n",
        "\n",
        "    def _load_local_llm(self):\n",
        "        \"\"\"Load the local LLM and tokenizer with proper attention mask handling\"\"\"\n",
        "        try:\n",
        "            print(f\"Loading {self.model_name}...\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "            # Set pad token if not set\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "            # Set padding side to left for better generation\n",
        "            tokenizer.padding_side = \"left\"\n",
        "\n",
        "            print(f\"Successfully loaded {self.model_name}\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {self.model_name}: {e}\")\n",
        "            print(\"Falling back to DialoGPT-large...\")\n",
        "            try:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    \"microsoft/DialoGPT-large\",\n",
        "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "                )\n",
        "                if tokenizer.pad_token is None:\n",
        "                    tokenizer.pad_token = tokenizer.eos_token\n",
        "                tokenizer.padding_side = \"left\"\n",
        "                return model, tokenizer\n",
        "            except Exception as e2:\n",
        "                print(f\"Error loading fallback model: {e2}\")\n",
        "                return None, None\n",
        "\n",
        "    def _read_json_file(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Read JSON file with aggressive repair for malformed JSON\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Try normal parsing first\n",
        "            try:\n",
        "                return json.loads(content)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON parsing failed: {e}\")\n",
        "                print(\"Attempting aggressive JSON repair...\")\n",
        "\n",
        "                # Aggressive JSON repair\n",
        "                repaired_content = self._repair_json(content)\n",
        "                return json.loads(repaired_content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"File reading failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _repair_json(self, content: str) -> str:\n",
        "        \"\"\"Aggressively repair malformed JSON\"\"\"\n",
        "        import re\n",
        "\n",
        "        # Fix 1: Remove trailing commas\n",
        "        content = re.sub(r',\\s*}', '}', content)\n",
        "        content = re.sub(r',\\s*]', ']', content)\n",
        "\n",
        "        # Fix 2: Add missing commas between objects/arrays\n",
        "        content = re.sub(r'}\\s*{', '}, {', content)\n",
        "        content = re.sub(r']\\s*\\[', '], [', content)\n",
        "        content = re.sub(r'}\\s*\\[', '}, [', content)\n",
        "        content = re.sub(r']\\s*{', '], {', content)\n",
        "\n",
        "        # Fix 3: Handle specific line 34558 issue - look for missing comma patterns\n",
        "        lines = content.split('\\n')\n",
        "        if len(lines) > 34557:\n",
        "            # Check the problematic line and surrounding context\n",
        "            problem_line = lines[34557]  # 0-indexed\n",
        "            print(f\"Problem line 34558: {repr(problem_line)}\")\n",
        "\n",
        "            # Try to fix common patterns on this line\n",
        "            if problem_line.strip().endswith('}') and not problem_line.strip().endswith(',}'):\n",
        "                # Look at the next line to see if it starts with {\n",
        "                if len(lines) > 34558 and lines[34558].strip().startswith('{'):\n",
        "                    lines[34557] = problem_line.rstrip() + ','\n",
        "                    content = '\\n'.join(lines)\n",
        "                    print(\"Fixed missing comma at line 34558\")\n",
        "\n",
        "        # Fix 4: More aggressive comma insertion\n",
        "        # Look for patterns like: } followed by { on next line\n",
        "        content = re.sub(r'}\\s*\\n\\s*{', '},\\n{', content)\n",
        "        content = re.sub(r']\\s*\\n\\s*\\[', '],\\n[', content)\n",
        "\n",
        "        # Fix 5: Handle unclosed strings or other issues\n",
        "        # This is a last resort - try to balance braces and brackets\n",
        "        open_braces = content.count('{') - content.count('}')\n",
        "        open_brackets = content.count('[') - content.count(']')\n",
        "\n",
        "        if open_braces > 0:\n",
        "            content += '}' * open_braces\n",
        "            print(f\"Added {open_braces} closing braces\")\n",
        "        if open_brackets > 0:\n",
        "            content += ']' * open_brackets\n",
        "            print(f\"Added {open_brackets} closing brackets\")\n",
        "\n",
        "        return content\n",
        "\n",
        "    def index_timing_reports(self, directory: str):\n",
        "        \"\"\"Index all JSON files in the directory\"\"\"\n",
        "        print(\"=== Indexing timing reports ===\")\n",
        "\n",
        "        json_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
        "        print(f\"Found {len(json_files)} JSON files to index\")\n",
        "\n",
        "        for filename in json_files:\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            print(f\"Indexing {filename}...\")\n",
        "\n",
        "            try:\n",
        "                data = self._read_json_file(file_path)\n",
        "                if not data:\n",
        "                    print(f\"Failed to read {filename}\")\n",
        "                    continue\n",
        "\n",
        "                # Extract text and metadata for each path\n",
        "                for i, path in enumerate(data.get('paths', [])):\n",
        "                    # Create a comprehensive text representation\n",
        "                    text_parts = []\n",
        "\n",
        "                    # Add path metadata\n",
        "                    if 'startpoint' in path:\n",
        "                        text_parts.append(f\"Startpoint: {path['startpoint'].get('instance', 'N/A')}\")\n",
        "                    if 'endpoint' in path:\n",
        "                        text_parts.append(f\"Endpoint: {path['endpoint'].get('instance', 'N/A')}\")\n",
        "                    if 'report' in path:\n",
        "                        report = path['report']\n",
        "                        text_parts.append(f\"Group: {report.get('group', 'N/A')}\")\n",
        "                        text_parts.append(f\"Path Type: {report.get('path_type', 'N/A')}\")\n",
        "\n",
        "                    # Add summary information\n",
        "                    if 'summary' in path:\n",
        "                        summary = path['summary']\n",
        "                        if 'slack' in summary:\n",
        "                            text_parts.append(f\"Slack: {summary['slack']}\")\n",
        "                        if 'hold_time_requirement' in summary and summary['hold_time_requirement'] is not None:\n",
        "                            text_parts.append(f\"Hold Time Requirement: {summary['hold_time_requirement']}\")\n",
        "                        if 'clock_skew' in summary and summary['clock_skew'] is not None:\n",
        "                            text_parts.append(f\"Clock Skew: {summary['clock_skew']}\")\n",
        "\n",
        "                    # Add launch clock path stages\n",
        "                    if 'launch_clock_path' in path and 'stages' in path['launch_clock_path']:\n",
        "                        text_parts.append(\"Launch Clock Path:\")\n",
        "                        for stage in path['launch_clock_path']['stages']:\n",
        "                            if stage.get('type') == 'cell_pin':\n",
        "                                text_parts.append(f\"  {stage.get('name', 'N/A')} ({stage.get('cell_type', 'N/A')})\")\n",
        "\n",
        "                    # Add data path stages\n",
        "                    if 'data_path' in path and 'stages' in path['data_path']:\n",
        "                        text_parts.append(\"Data Path:\")\n",
        "                        for stage in path['data_path']['stages']:\n",
        "                            if stage.get('type') == 'cell_pin':\n",
        "                                text_parts.append(f\"  {stage.get('name', 'N/A')} ({stage.get('cell_type', 'N/A')})\")\n",
        "\n",
        "                    # Add capture clock path stages\n",
        "                    if 'capture_clock_path' in path and 'stages' in path['capture_clock_path']:\n",
        "                        text_parts.append(\"Capture Clock Path:\")\n",
        "                        for stage in path['capture_clock_path']['stages']:\n",
        "                            if stage.get('type') == 'cell_pin':\n",
        "                                text_parts.append(f\"  {stage.get('name', 'N/A')} ({stage.get('cell_type', 'N/A')})\")\n",
        "\n",
        "                    # Combine all text\n",
        "                    full_text = \"\\n\".join(text_parts)\n",
        "\n",
        "                    # Generate embedding\n",
        "                    embedding = self.embedding_model.encode(full_text).tolist()\n",
        "\n",
        "                    # Store in ChromaDB\n",
        "                    self.collection.add(\n",
        "                        embeddings=[embedding],\n",
        "                        documents=[full_text],\n",
        "                        metadatas=[{\n",
        "                            'filename': filename,\n",
        "                            'path_index': i,\n",
        "                            'startpoint': path.get('startpoint', {}).get('instance', 'N/A'),\n",
        "                            'endpoint': path.get('endpoint', {}).get('instance', 'N/A'),\n",
        "                            'slack': path.get('summary', {}).get('slack', 'N/A'),\n",
        "                            'hold_time_requirement': path.get('summary', {}).get('hold_time_requirement', 'N/A'),\n",
        "                            'clock_skew': path.get('summary', {}).get('clock_skew', 'N/A'),\n",
        "                            'group': path.get('report', {}).get('group', 'N/A')\n",
        "                        }],\n",
        "                        ids=[f\"{filename}_{i}\"]\n",
        "                    )\n",
        "\n",
        "                print(f\"Successfully indexed {filename} with {len(data.get('paths', []))} paths\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error indexing {filename}: {e}\")\n",
        "\n",
        "        print(\"Indexing complete!\")\n",
        "\n",
        "    def _get_all_slack_data(self) -> List[Dict]:\n",
        "        \"\"\"Get all slack-related data from the collection\"\"\"\n",
        "        print(\"DEBUG: Getting all slack data from collection...\")\n",
        "        all_data = []\n",
        "\n",
        "        try:\n",
        "            print(\"DEBUG: Calling collection.get()...\")\n",
        "            results = self.collection.get()\n",
        "            print(f\"DEBUG: Collection.get() returned {len(results.get('metadatas', []))} entries\")\n",
        "\n",
        "            for i, metadata in enumerate(results['metadatas']):\n",
        "                if metadata.get('slack') != 'N/A':\n",
        "                    try:\n",
        "                        slack_value = float(metadata['slack'])\n",
        "                        all_data.append({\n",
        "                            'slack': slack_value,\n",
        "                            'startpoint': metadata.get('startpoint', 'N/A'),\n",
        "                            'endpoint': metadata.get('endpoint', 'N/A'),\n",
        "                            'group': metadata.get('group', 'N/A'),\n",
        "                            'hold_time_requirement': metadata.get('hold_time_requirement', 'N/A'),\n",
        "                            'clock_skew': metadata.get('clock_skew', 'N/A'),\n",
        "                            'document': results['documents'][i],\n",
        "                            'path_index': metadata.get('path_index', i)\n",
        "                        })\n",
        "                    except (ValueError, TypeError):\n",
        "                        continue\n",
        "\n",
        "            print(f\"DEBUG: Processed {len(all_data)} slack data entries\")\n",
        "            return all_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Error in _get_all_slack_data: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _get_clock_skew_data(self) -> List[Dict]:\n",
        "        \"\"\"Get all clock skew data from the collection\"\"\"\n",
        "        all_data = []\n",
        "        results = self.collection.get()\n",
        "\n",
        "        for i, metadata in enumerate(results['metadatas']):\n",
        "            if metadata.get('clock_skew') != 'N/A':\n",
        "                try:\n",
        "                    clock_skew_value = float(metadata['clock_skew'])\n",
        "                    all_data.append({\n",
        "                        'clock_skew': clock_skew_value,\n",
        "                        'startpoint': metadata.get('startpoint', 'N/A'),\n",
        "                        'endpoint': metadata.get('endpoint', 'N/A'),\n",
        "                        'group': metadata.get('group', 'N/A'),\n",
        "                        'slack': metadata.get('slack', 'N/A'),\n",
        "                        'document': results['documents'][i],\n",
        "                        'path_index': metadata.get('path_index', i)\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def _get_hold_time_data(self) -> List[Dict]:\n",
        "        \"\"\"Get all hold time requirement data from the collection\"\"\"\n",
        "        all_data = []\n",
        "        results = self.collection.get()\n",
        "\n",
        "        for i, metadata in enumerate(results['metadatas']):\n",
        "            if metadata.get('hold_time_requirement') != 'N/A':\n",
        "                try:\n",
        "                    hold_time_value = float(metadata['hold_time_requirement'])\n",
        "                    all_data.append({\n",
        "                        'hold_time_requirement': hold_time_value,\n",
        "                        'startpoint': metadata.get('startpoint', 'N/A'),\n",
        "                        'endpoint': metadata.get('endpoint', 'N/A'),\n",
        "                        'group': metadata.get('group', 'N/A'),\n",
        "                        'slack': metadata.get('slack', 'N/A'),\n",
        "                        'document': results['documents'][i],\n",
        "                        'path_index': metadata.get('path_index', i)\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def _classify_question_with_llm(self, question: str) -> str:\n",
        "        \"\"\"Use CodeLlama to classify the question type with timeout\"\"\"\n",
        "        import time\n",
        "\n",
        "        if not self.llm_model or not self.tokenizer:\n",
        "            print(\"DEBUG: LLM not available, returning 'general'\")\n",
        "            return \"general\"\n",
        "\n",
        "        print(f\"DEBUG: LLM classification for: '{question}'\")\n",
        "\n",
        "        # Much simpler prompt for faster classification\n",
        "        prompt = f\"\"\"<s>[INST] Classify this timing question:\n",
        "\n",
        "- ranking: worst/best/top questions\n",
        "- counting: how many/total questions\n",
        "- filtering: show/find questions\n",
        "- general: other questions\n",
        "\n",
        "Question: \"{question}\"\n",
        "\n",
        "Category: [/INST]\"\"\"\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            print(\"DEBUG: Tokenizing classification prompt...\")\n",
        "            inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)  # Shorter\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            print(\"DEBUG: Creating attention mask...\")\n",
        "            # Create attention mask\n",
        "            attention_mask = (inputs != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "            print(\"DEBUG: Starting LLM classification generation...\")\n",
        "            with torch.no_grad():\n",
        "                outputs = self.llm_model.generate(\n",
        "                    inputs,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_new_tokens=3,  # Very short for classification\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.1,\n",
        "                    do_sample=False,  # Deterministic for speed\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            end_time = time.time()\n",
        "            print(f\"DEBUG: LLM generation took {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "            print(\"DEBUG: Decoding classification response...\")\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            category = response.split(\"[/INST]\")[-1].strip().lower()\n",
        "\n",
        "            print(f\"DEBUG: Raw LLM response: '{response}'\")\n",
        "            print(f\"DEBUG: Extracted category: '{category}'\")\n",
        "\n",
        "            # Clean up the response\n",
        "            category = category.replace(\"category:\", \"\").strip()\n",
        "\n",
        "            # Map to high-level categories\n",
        "            category = category.strip().lower()\n",
        "            if category in [\"ranking\", \"counting\", \"statistics\", \"filtering\", \"navigation\", \"general\"]:\n",
        "                print(f\"DEBUG: Valid category found: '{category}'\")\n",
        "                return category\n",
        "            else:\n",
        "                print(f\"DEBUG: Invalid category '{category}', returning 'general'\")\n",
        "                return \"general\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in LLM question classification: {e}\")\n",
        "            return \"general\"\n",
        "\n",
        "    def _detect_query_complexity(self, question: str) -> str:\n",
        "        \"\"\"Detect if question needs LLM processing or direct processing\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Simple data retrieval patterns\n",
        "        simple_patterns = [\n",
        "            \"worst\", \"best\", \"top\", \"bottom\", \"highest\", \"lowest\",  # Basic ranking\n",
        "            \"total\", \"count\", \"number of\",                           # Simple counts (NO conditions)\n",
        "            \"how many\",                                              # counting questions\n",
        "            \"paths are\", \"are there\", \"paths total\",                # Simple counting variations\n",
        "            \"average\", \"mean\", \"statistics\", \"distribution\",       # Basic statistics\n",
        "            \"clock skew\", \"skew\", \"hold time\",                     # Specific metrics\n",
        "            \"startpoint\", \"endpoint\", \"show\",                       # Data display\n",
        "            \"next\", \"previous\"                                      # Navigation\n",
        "        ]\n",
        "\n",
        "        # Simple timing status patterns (can be handled fast)\n",
        "        timing_status_patterns = [\n",
        "            \"failing\", \"violation\", \"fail\", \"failed\",            # Timing status\n",
        "            \"passing\", \"pass\", \"passed\",                          # Timing status\n",
        "            \"critical\", \"borderline\"                             # Timing status\n",
        "        ]\n",
        "\n",
        "        # Complex analytical patterns\n",
        "        complex_patterns = [\n",
        "            \"why\", \"why is\", \"why are\", \"reason\", \"cause\",         # Explanations\n",
        "            \"analyze\", \"analysis\", \"pattern\", \"correlation\",       # Analysis\n",
        "            \"compare\", \"difference\", \"between\",                    # Comparisons\n",
        "            \"what if\", \"impact\", \"effect\",                         # Causal analysis\n",
        "            \"less than\", \"greater than\", \"more than\", \"between\",   # Conditional queries\n",
        "            \"above\", \"below\", \"equal to\", \"higher than\", \"lower than\",  # Additional conditions\n",
        "            \"can slacks\", \"if clock\", \"relationship\", \"how does\",  # Technical reasoning\n",
        "            \"positive clock\", \"negative clock\", \"skew effect\"      # Clock skew analysis\n",
        "        ]\n",
        "\n",
        "        # Recommendation patterns (should handle with direct logic, not LLM)\n",
        "        recommendation_patterns = [\n",
        "            \"recommend\", \"suggest\", \"optimize\", \"improve\",\n",
        "            \"how can we\", \"what can we do\", \"how to improve\",\n",
        "            \"make better\", \"improve slack\", \"better slack\"\n",
        "        ]\n",
        "\n",
        "        # Check for simple timing status patterns first\n",
        "        for pattern in timing_status_patterns:\n",
        "            if pattern in question_lower:\n",
        "                return \"simple\"\n",
        "\n",
        "        # Check for recommendation patterns (handle with direct logic)\n",
        "        for pattern in recommendation_patterns:\n",
        "            if pattern in question_lower:\n",
        "                print(f\"DEBUG: Detected recommendation pattern '{pattern}', forcing simple classification\")\n",
        "                return \"simple\"\n",
        "\n",
        "        # Force simple classification for certain problematic phrases\n",
        "        problematic_phrases = [\n",
        "            \"reason for worst slack\",\n",
        "            \"why worst slack\",\n",
        "            \"worst slack reason\"\n",
        "        ]\n",
        "        for phrase in problematic_phrases:\n",
        "            if phrase in question_lower:\n",
        "                print(f\"DEBUG: Detected problematic phrase '{phrase}', forcing simple classification\")\n",
        "                return \"simple\"\n",
        "\n",
        "        # Check for complex indicators\n",
        "        for pattern in complex_patterns:\n",
        "            if pattern in question_lower:\n",
        "                return \"complex\"\n",
        "\n",
        "        # Check for simple indicators\n",
        "        for pattern in simple_patterns:\n",
        "            if pattern in question_lower:\n",
        "                return \"simple\"\n",
        "\n",
        "        return \"complex\"  # Default to complex for safety\n",
        "\n",
        "    def _classify_question(self, question: str, data: List[Dict]) -> str:\n",
        "        \"\"\"Smart classification: route between fast and LLM processing\"\"\"\n",
        "        print(f\"DEBUG: Classifying question: '{question}'\")\n",
        "\n",
        "        # Detect complexity first\n",
        "        complexity = self._detect_query_complexity(question)\n",
        "        print(f\"DEBUG: Query complexity: {complexity}\")\n",
        "\n",
        "        if complexity == \"simple\":\n",
        "            print(\"DEBUG: Using fast pattern matching for simple query...\")\n",
        "            question_lower = question.lower()\n",
        "\n",
        "            # Ranking patterns\n",
        "            if any(word in question_lower for word in [\"worst\", \"best\", \"top\", \"bottom\", \"highest\", \"lowest\", \"rank\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'ranking'\")\n",
        "                return \"ranking\"\n",
        "            # Counting patterns\n",
        "            elif any(word in question_lower for word in [\"how many\", \"total\", \"count\", \"number of\", \"are there\", \"paths are\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'counting'\")\n",
        "                return \"counting\"\n",
        "            # Statistics patterns\n",
        "            elif any(word in question_lower for word in [\"average\", \"mean\", \"median\", \"statistics\", \"distribution\", \"range\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'statistics'\")\n",
        "                return \"statistics\"\n",
        "            # Clock skew patterns\n",
        "            elif any(word in question_lower for word in [\"clock skew\", \"skew\", \"clock_skew\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'filtering' (clock skew)\")\n",
        "                return \"filtering\"\n",
        "            # Hold time patterns\n",
        "            elif any(word in question_lower for word in [\"hold time\", \"hold_time\", \"hold time requirement\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'filtering' (hold time)\")\n",
        "                return \"filtering\"\n",
        "            # Timing status patterns\n",
        "            elif any(word in question_lower for word in [\"failing\", \"violation\", \"crash\", \"crash\", \"passing\", \"pass\", \"critical\", \"borderline\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'filtering' (timing status)\")\n",
        "                return \"filtering\"\n",
        "            # Recommendation patterns\n",
        "            elif any(word in question_lower for word in [\"recommend\", \"suggest\", \"optimize\", \"improve\", \"how can we\", \"what can we do\", \"make better\", \"better slack\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'filtering' (recommendations)\")\n",
        "                return \"filtering\"\n",
        "            # Filtering patterns\n",
        "            elif any(word in question_lower for word in [\"startpoint\", \"endpoint\", \"show\", \"find\", \"search\", \"filter\", \"where\", \"which\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'filtering'\")\n",
        "                return \"filtering\"\n",
        "            # Navigation patterns\n",
        "            elif any(word in question_lower for word in [\"next\", \"previous\", \"browse\", \"iterate\", \"go to\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'navigation'\")\n",
        "                return \"navigation\"\n",
        "            else:\n",
        "                print(\"DEBUG: Pattern matched as 'general'\")\n",
        "                return \"general\"\n",
        "        else:\n",
        "            # Complex query - return 'complex' category\n",
        "            print(f\"DEBUG: Complex query detected: '{question}', returning 'complex'\")\n",
        "            return \"complex\"\n",
        "\n",
        "    def _handle_ranking_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle ranking queries with metric detection\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data available.\"\n",
        "\n",
        "        print(f\"DEBUG: Handling ranking query: {question}\")\n",
        "        print(f\"DEBUG: Found {len(all_slack_data)} paths\")\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Check for specific metric requests\n",
        "        if \"clock skew\" in question_lower or \"skew\" in question_lower:\n",
        "            print(f\"DEBUG: Clock skew ranking requested\")\n",
        "            return self._handle_clock_skew_ranking(question, all_slack_data)\n",
        "        elif \"hold time\" in question_lower or \"hold_time\" in question_lower:\n",
        "            print(f\"DEBUG: Hold time ranking requested\")\n",
        "            return self._handle_hold_time_ranking(question, all_slack_data)\n",
        "\n",
        "        # Default to slack ranking\n",
        "        # Extract number from question for fallback\n",
        "        n = self._extract_number_from_question(question)\n",
        "        print(f\"DEBUG: Extracted number: {n}\")\n",
        "\n",
        "        # Skip LLM entirely - it's too slow!\n",
        "        print(f\"DEBUG: Using direct processing (no LLM - it's too slow!)\")\n",
        "        return self._handle_ranking_fallback(question, all_slack_data, n)\n",
        "\n",
        "    def _handle_ranking_fallback(self, question: str, all_slack_data: List[Dict], n: int) -> str:\n",
        "        \"\"\"Fallback ranking handler without LLM\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Check if asking for reasons/analysis specifically\n",
        "        if \"reason\" in question_lower or (\"why\" in question_lower and \"worst\" in question_lower):\n",
        "            return self._analyze_worst_slack_reasons(all_slack_data)\n",
        "\n",
        "        # Sort paths by slack\n",
        "        if \"worst\" in question_lower or \"bottom\" in question_lower:\n",
        "            sorted_paths = sorted(all_slack_data, key=lambda x: x['slack'])\n",
        "            direction = \"worst\"\n",
        "        else:\n",
        "            sorted_paths = sorted(all_slack_data, key=lambda x: x['slack'], reverse=True)\n",
        "            direction = \"best\"\n",
        "\n",
        "        # Get top N paths\n",
        "        n = min(n, len(all_slack_data))\n",
        "        top_paths = sorted_paths[:n]\n",
        "\n",
        "        result = f\"Top {n} {direction} slack paths:\\n\"\n",
        "        for i, path in enumerate(top_paths, 1):\n",
        "            result += f\"{i}. Slack: {path['slack']}ns - Path from {path['startpoint']} to {path['endpoint']} in group {path['group']}\\n\"\n",
        "\n",
        "        return result.strip()\n",
        "\n",
        "    def _analyze_worst_slack_reasons(self, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Analyze reasons for worst slack\"\"\"\n",
        "        worst_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "        second_worst = sorted(all_slack_data, key=lambda x: x['slack'])[1] if len(all_slack_data) > 1 else worst_path\n",
        "\n",
        "        analysis_parts = []\n",
        "\n",
        "        # Analyze the worst path\n",
        "        analysis_parts.append(f\"📊 WORST SLACK ANALYSIS:\")\n",
        "        analysis_parts.append(f\"📍 Worst path: {worst_path['slack']:.3f}ns\")\n",
        "        analysis_parts.append(f\"🔄 From: {worst_path['startpoint']} → {worst_path['endpoint']}\")\n",
        "\n",
        "        # Clock skew analysis\n",
        "        clock_skew = worst_path.get('clock_skew', 'N/A')\n",
        "        if clock_skew != 'N/A':\n",
        "            analysis_parts.append(f\"⏰ Clock skew: {clock_skew:.3f}ns\")\n",
        "            try:\n",
        "                if float(clock_skew) > 0.4:\n",
        "                    analysis_parts.append(\"   ⚠️ HIGH clock skew - impacts timing margin\")\n",
        "                else:\n",
        "                    analysis_parts.append(\"   ✅ Moderate clock skew\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Hold time analysis\n",
        "        hold_time = worst_path.get('hold_time_requirement', 'N/A')\n",
        "        if hold_time != 'N/A':\n",
        "            analysis_parts.append(f\"🔒 Hold time requirement: {hold_time}ns\")\n",
        "            try:\n",
        "                if float(hold_time) > 0.1:\n",
        "                    analysis_parts.append(\"   ⚠️ HIGH hold time requirement\")\n",
        "                else:\n",
        "                    analysis_parts.append(\"   ✅ Normal hold time requirement\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Comparative analysis\n",
        "        analysis_parts.append(f\"\\n📈 COMPARATIVE ANALYSIS:\")\n",
        "        analysis_parts.append(f\"🎯 Slack range: {worst_path['slack']:.3f}ns to {second_worst['slack']:.3f}ns\")\n",
        "        analysis_parts.append(f\"💰 Margin difference: {second_worst['slack'] - worst_path['slack']:.3f}ns\")\n",
        "\n",
        "        # Summary assessment\n",
        "        analysis_parts.append(f\"\\n✅ TIMING STATUS:\")\n",
        "        analysis_parts.append(f\"🎯 Both paths PASS timing (positive slack)\")\n",
        "        analysis_parts.append(f\"⚠️ Small margins indicate timing sensitivity\")\n",
        "        analysis_parts.append(f\"💡 Consider design optimizations for robustness\")\n",
        "\n",
        "        return \"\\n\".join(analysis_parts)\n",
        "\n",
        "    def _generate_slack_improvement_recommendations(self, all_slack_data: List[Dict], question: str) -> str:\n",
        "        \"\"\"Generate comprehensive slack improvement recommendations based on actual data\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available for analysis.\"\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Determine number of recommendations requested\n",
        "        num_points = 10  # Default\n",
        "        if \"5 points\" in question_lower or \"5 point\" in question_lower:\n",
        "                num_points = 5\n",
        "        elif \"3 points\" in question_lower or \"3 point\" in question_lower:\n",
        "            num_points = 3\n",
        "\n",
        "        worst_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "        best_path = max(all_slack_data, key=lambda x: x['slack'])\n",
        "\n",
        "        recommendations = []\n",
        "\n",
        "        # Analyze current timing situation\n",
        "        slack_values = [p['slack'] for p in all_slack_data]\n",
        "        avg_slack = sum(slack_values) / len(slack_values)\n",
        "        min_slack = min(slack_values)\n",
        "\n",
        "        recommendations.append(f\"📊 CURRENT TIMING STATUS:\")\n",
        "        recommendations.append(f\"• Worst slack: {worst_path['slack']:.3f}ns\")\n",
        "        recommendations.append(f\"• Best slack: {best_path['slack']:.3f}ns\")\n",
        "        recommendations.append(f\"• Average slack: {avg_slack:.3f}ns\")\n",
        "        recommendations.append(f\"• All paths PASS timing ✅\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        recommendations.append(f\"🎯 TOP {num_points} SLACK IMPROVEMENT RECOMMENDATIONS:\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        # Clock skew optimization\n",
        "        clock_skews = [p.get('clock_skew', 0) for p in all_slack_data if p.get('clock_skew') != 'N/A']\n",
        "        if clock_skews and max(clock_skews) > 0.4:\n",
        "            recommendations.append(f\"1. 🔧 BALANCE CLOCK SKEW\")\n",
        "            recommendations.append(f\"   Current worst skew: {max(clock_skews):.3f}ns\")\n",
        "            recommendations.append(f\"   → Implement balanced clock distribution\")\n",
        "            recommendations.append(f\"   → Add buffer cells in high-skew regions\")\n",
        "            recommendations.append(f\"   → Optimize clock tree synthesis\")\n",
        "            recommendations.append(\"\")\n",
        "\n",
        "        # Hold time optimization\n",
        "        hold_times = [p.get('hold_time_requirement', 0) for p in all_slack_data if p.get('hold_time_requirement') != 'N/A']\n",
        "        if hold_times and max(hold_times) > 0.1:\n",
        "            recommendations.append(f\"2. ⏰ OPTIMIZE HOLD TIME REQUIREMENTS\")\n",
        "            recommendations.append(f\"   Current worst hold time: {max(hold_times):.3f}ns\")\n",
        "            recommendations.append(f\"   → Add hold buffers in critical paths\")\n",
        "            recommendations.append(f\"   → Optimize flip-flop timing\")\n",
        "            recommendations.append(\"\")\n",
        "\n",
        "        # Design optimization based on path analysis\n",
        "        recommendations.append(f\"{2 if clock_skews and max(clock_skews) > 0.4 else 3 if hold_times and max(hold_times) > 0.1 else 1}. 📐 OPTIMIZE DESIGN TOPOLOGY\")\n",
        "        recommendations.append(f\"   Worst path: {worst_path['startpoint']} → {worst_path['endpoint']}\")\n",
        "        if \"housekeeping\" in worst_path.get('startpoint', ''):\n",
        "            recommendations.append(f\"   → Housekeeping modules often have timing sensitivity\")\n",
        "            recommendations.append(f\"   → Consider register pipelining\")\n",
        "        recommendations.append(f\"   → Review logic synthesis constraints\")\n",
        "        recommendations.append(f\"   → Optimize wire routing and placement\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        # Slack margin improvement\n",
        "        recommendations.append(f\"{3 if clock_skews and max(clock_skews) > 0.4 else 4 if hold_times and max(hold_times) > 0.1 else 2}. 📈 IMPROVE TIMING MARGINS\")\n",
        "        recommendations.append(f\"   Current margins: {min_slack:.3f}ns to {max(slack_values):.3f}ns\")\n",
        "        recommendations.append(f\"   → Target minimum slack > 0.5ns for robustness\")\n",
        "        recommendations.append(f\"   → Consider operating condition guardbands\")\n",
        "        recommendations.append(f\"   → Add timing margin in synthesis\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        # Speed grade optimization\n",
        "        recommendations.append(f\"{4 if clock_skews and max(clock_skews) > 0.4 else 5 if hold_times and max(hold_times) > 0.1 else 3}. 🚀 SPEED GRADE OPTIMIZATION\")\n",
        "        recommendations.append(f\"   → Evaluate slower speed grades for better timing\")\n",
        "        recommendations.append(f\"   → Trade-off performance vs reliability\")\n",
        "        recommendations.append(f\"   → Consider multi-clock domain partitioning\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        # Temperature and voltage optimization\n",
        "        recommendations.append(f\"{5 if clock_skews and max(clock_skews) > 0.4 else 6 if hold_times and max(hold_times) > 0.1 else 4}. 🌡️ TEMPERATURE/VOLTAGE ANALYSIS\")\n",
        "        recommendations.append(f\"   → Analyze timing across temperature corners\")\n",
        "        recommendations.append(f\"   → Consider voltage scaling optimization\")\n",
        "        recommendations.append(f\"   → Review process corner sensitivity\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        # Add remaining recommendations to reach requested count\n",
        "        remaining_count = num_points - (5 if clock_skews and max(clock_skews) > 0.4 else 5 if hold_times and max(hold_times) > 0.1 else 5)\n",
        "\n",
        "        additional_recommendations = [\n",
        "            (\"6. 📋 DESIGN RULE OPTIMIZATION\", \"→ Minimize long wires\\n→ Add repeaters in nets > threshold\\n→ Optimize fanout distribution\"),\n",
        "            (\"7. 🔄 LOGIC OPTIMIZATION\", \"→ Use carry chains efficiently\\n→ Balance combinational logic\\n→ Pipeline critical sections\"),\n",
        "            (\"8. ⚡ POWER OPTIMIZATION\", \"→ Clock gating for unused blocks\\n→ Dynamic voltage scaling\\n→ Reduce switching activity\"),\n",
        "            (\"9. 🎯 CONSTRAINT REFINEMENT\", \"→ Review timing constraints\\n→ Add false/multicycle paths\\n→ Optimize I/O timing\"),\n",
        "            (\"10. 🧪 ANALYSIS IMPROVEMENT\", \"#  → Run Monte Carlo analysis\\n→ Add statistical timing\\n→ Review design coverage\")\n",
        "        ]\n",
        "\n",
        "        for i in range(min(remaining_count, len(additional_recommendations))):\n",
        "            # Calculate proper numbering based on previously added recommendations\n",
        "            base_num = 5\n",
        "            if clock_skews and max(clock_skews) > 0.4:\n",
        "                base_num = 6\n",
        "            elif hold_times and max(hold_times) > 0.1:\n",
        "                base_num = 6\n",
        "            else:\n",
        "                base_num = 4\n",
        "\n",
        "            idx = base_num + i\n",
        "            rec = additional_recommendations[i]\n",
        "            recommendations.append(f\"{idx}. {rec[0]}\")\n",
        "            recommendations.append(f\"   {rec[1]}\")\n",
        "            recommendations.append(\"\")\n",
        "\n",
        "        recommendations.append(\"⚠️  IMPORTANT: Current design PASSES timing. These recommendations optimize margins for robustness.\")\n",
        "\n",
        "        return \"\\n\".join(recommendations)\n",
        "\n",
        "    def _generate_technical_reasoning(self, all_slack_data: List[Dict], question: str) -> str:\n",
        "        \"\"\"Generate technical reasoning about timing relationships\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available for analysis.\"\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Clock skew vs slack analysis\n",
        "        if \"clock skew\" in question_lower or \"skew\" in question_lower:\n",
        "            return self._explain_clock_skew_relationship(all_slack_data, question)\n",
        "\n",
        "        # General timing relationships\n",
        "        return self._explain_timing_relationships(all_slack_data, question)\n",
        "\n",
        "    def _explain_clock_skew_relationship(self, all_slack_data: List[Dict], question: str) -> str:\n",
        "        \"\"\"Explain the relationship between clock skew and slack\"\"\"\n",
        "        worst_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "        best_path = max(all_slack_data, key=lambda x: x['slack'])\n",
        "\n",
        "        # Get actual clock skew values\n",
        "        clock_skews = [p.get('clock_skew', 0) for p in all_slack_data if p.get('clock_skew') != 'N/A']\n",
        "        current_skew = clock_skews[0] if clock_skews else 0.5  # Use first available skew\n",
        "\n",
        "        explanation = []\n",
        "        explanation.append(\"🔬 CLOCK SKEW vs SLACK RELATIONSHIP:\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"📊 CURRENT DATA:\")\n",
        "        explanation.append(f\"• Current clock skew: {current_skew:.3f}ns\")\n",
        "        explanation.append(f\"• Worst slack: {worst_path['slack']:.3f}ns\")\n",
        "        explanation.append(f\"• Best slack: {best_path['slack']:.3f}ns\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"🧠 TECHNICAL EXPLANATION:\")\n",
        "        explanation.append(\"\")\n",
        "        explanation.append(\"Clock skew affects slack through timing paths:\")\n",
        "        explanation.append(\"\")\n",
        "        explanation.append(\"🔸 POSITIVE CLOCK SKEW (launch clock arrives LATER):\")\n",
        "        explanation.append(\"   • Data has MORE time to propagate\")\n",
        "        explanation.append(\"   • → SLACK IMPROVES (becomes more positive)\")\n",
        "        explanation.append(\"   • → Better timing margin\")\n",
        "        explanation.append(\"\")\n",
        "        explanation.append(\"🔸 NEGATIVE CLOCK SKEW (launch clock arrives EARLIER):\")\n",
        "        explanation.append(\"   • Data has LESS time to propagate\")\n",
        "        explanation.append(\"   • → SLACK DEGRADES (becomes less positive)\")\n",
        "        explanation.append(\"   • → Worse timing margin\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"📐 MATHEMATICAL RELATIONSHIP:\")\n",
        "        explanation.append(\"Slack = Required_Time - Arrival_Time\")\n",
        "        explanation.append(\"If clock_skew increases (more positive):\")\n",
        "        explanation.append(\"→ Required_Time increases\")\n",
        "        explanation.append(\"→ Slack = (Required_Time + skew) - Arrival_Time\")\n",
        "        explanation.append(\"→ Slack IMPROVES ✅\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"🎯 ANSWER TO YOUR QUESTION:\")\n",
        "        explanation.append(\"If clock skews become MORE POSITIVE:\")\n",
        "        explanation.append(\"→ SLACKS WILL IMPROVE (become more positive)\")\n",
        "        explanation.append(\"→ Better timing margins\")\n",
        "        explanation.append(\"→ More robust design\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"⚠️  PRACTICAL CONSIDERATIONS:\")\n",
        "        explanation.append(\"• Positive skew helps setup timing\")\n",
        "        explanation.append(\"• But may hurt hold timing\")\n",
        "        explanation.append(\"• Balance between setup and hold is critical\")\n",
        "        explanation.append(\"• Current design shows moderate skew (0.5ns)\")\n",
        "\n",
        "        return \"\\n\".join(explanation)\n",
        "\n",
        "    def _explain_timing_relationships(self, all_slack_data: List[Dict], question: str) -> str:\n",
        "        \"\"\"General timing relationship explanations\"\"\"\n",
        "        explanation = []\n",
        "        explanation.append(\"🔬 TIMING RELATIONSHIPS EXPLANATION:\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        # Basic timing concepts\n",
        "        explanation.append(\"📚 FUNDAMENTAL TIMING CONCEPTS:\")\n",
        "        explanation.append(\"• Setup Time: Data must be stable before clock edge\")\n",
        "        explanation.append(\"• Hold Time: Data must remain stable after clock edge\")\n",
        "        explanation.append(\"• Slack: Margin between arrival and required time\")\n",
        "        explanation.append(\"• Clock Skew: Difference in clock arrival times\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"🎯 KEY RELATIONSHIPS:\")\n",
        "        explanation.append(\"• More positive slack = Better timing margin\")\n",
        "        explanation.append(\"• Clock skew affects both setup and hold timing\")\n",
        "        explanation.append(\"• Temperature/voltage variations impact timing\")\n",
        "        explanation.append(\"• Process corners affect all timing parameters\")\n",
        "\n",
        "        return \"\\n\".join(explanation)\n",
        "\n",
        "    def _handle_clock_skew_ranking(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle clock skew ranking queries\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available.\"\n",
        "\n",
        "        # Extract number from question\n",
        "        n = self._extract_number_from_question(question)\n",
        "\n",
        "        # Filter paths that have clock skew data\n",
        "        paths_with_skew = []\n",
        "        for path in all_slack_data:\n",
        "            if path.get('clock_skew') != 'N/A':\n",
        "                try:\n",
        "                    clock_skew = float(path['clock_skew'])\n",
        "                    paths_with_skew.append({\n",
        "                        'clock_skew': clock_skew,\n",
        "                        'startpoint': path['startpoint'],\n",
        "                        'endpoint': path['endpoint'],\n",
        "                        'group': path['group'],\n",
        "                        'slack': path['slack']\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        if not paths_with_skew:\n",
        "            return \"No clock skew data found in timing reports\"\n",
        "\n",
        "        # Sort by clock skew (default to worst = highest skew)\n",
        "        question_lower = question.lower()\n",
        "        if \"best\" in question_lower:\n",
        "            sorted_paths = sorted(paths_with_skew, key=lambda x: x['clock_skew'])\n",
        "            direction = \"best\"\n",
        "        else:\n",
        "            sorted_paths = sorted(paths_with_skew, key=lambda x: x['clock_skew'], reverse=True)\n",
        "            direction = \"worst\"\n",
        "\n",
        "        # Get top N paths\n",
        "        top_paths = sorted_paths[:min(n, len(sorted_paths))]\n",
        "\n",
        "        result = f\"Top {len(top_paths)} {direction} clock skew paths:\\n\"\n",
        "        for i, path in enumerate(top_paths, 1):\n",
        "            skew_status = \"⚠️ HIGH\" if path['clock_skew'] > 0.4 else \"📍 NORMAL\"\n",
        "            result += f\"{i}. Clock skew: {path['clock_skew']:.3f}ns [{skew_status}] - Path from {path['startpoint']} to {path['endpoint']} in group {path['group']} (Slack: {path['slack']:.3f}ns)\\n\"\n",
        "\n",
        "        return result.strip()\n",
        "\n",
        "    def _handle_hold_time_ranking(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle hold time ranking queries\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available.\"\n",
        "\n",
        "        # Extract number from question\n",
        "        n = self._extract_number_from_question(question)\n",
        "\n",
        "        # Filter paths that have hold time data\n",
        "        paths_with_hold_time = []\n",
        "        for path in all_slack_data:\n",
        "            if path.get('hold_time_requirement') != 'N/A':\n",
        "                try:\n",
        "                    hold_time = float(path['hold_time_requirement'])\n",
        "                    paths_with_hold_time.append({\n",
        "                        'hold_time_requirement': hold_time,\n",
        "                        'startpoint': path['startpoint'],\n",
        "                        'endpoint': path['endpoint'],\n",
        "                        'group': path['group'],\n",
        "                        'slack': path['slack']\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        if not paths_with_hold_time:\n",
        "            return \"No hold time requirement data found in timing reports\"\n",
        "\n",
        "        # Sort by hold time (default to worst = highest hold time)\n",
        "        question_lower = question.lower()\n",
        "        if \"best\" in question_lower:\n",
        "            sorted_paths = sorted(paths_with_hold_time, key=lambda x: x['hold_time_requirement'])\n",
        "            direction = \"best\"\n",
        "        else:\n",
        "            sorted_paths = sorted(paths_with_hold_time, key=lambda x: x['hold_time_requirement'], reverse=True)\n",
        "            direction = \"worst\"\n",
        "\n",
        "        # Get top N paths\n",
        "        top_paths = sorted_paths[:min(n, len(sorted_paths))]\n",
        "\n",
        "        result = f\"Top {len(top_paths)} {direction} hold time requirement paths:\\n\"\n",
        "        for i, path in enumerate(top_paths, 1):\n",
        "            hold_status = \"⚠️ HIGH\" if path['hold_time_requirement'] > 0.1 else \"📍 NORMAL\"\n",
        "            result += f\"{i}. Hold time: {path['hold_time_requirement']:.3f}ns [{hold_status}] - Path from {path['startpoint']} to {path['endpoint']} in group {path['group']} (Slack: {path['slack']:.3f}ns)\\n\"\n",
        "\n",
        "        return result.strip()\n",
        "\n",
        "    def _handle_counting_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle counting queries with condition parsing\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data available.\"\n",
        "\n",
        "        print(f\"DEBUG: Using direct counting (no LLM)...\")\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Handle simple total counts first\n",
        "        if any(phrase in question_lower for phrase in [\"are there\", \"paths are\", \"how many\", \"total paths\", \"paths total\"]):\n",
        "            # Simple counting - just return total\n",
        "            return f\"Total number of timing paths: {len(all_slack_data)}\"\n",
        "\n",
        "        # Check for conditional counts\n",
        "        if \"slack\" in question_lower:\n",
        "            # Parse conditions like \"slack less than 1ns\", \"slack greater than 1ns\"\n",
        "            if \"less than\" in question_lower or \"below\" in question_lower or \"<\" in question_lower:\n",
        "                # Extract threshold value\n",
        "                threshold = self._extract_threshold_from_question(question, \"less\")\n",
        "                if threshold is not None:\n",
        "                    count = sum(1 for path in all_slack_data if path['slack'] < threshold)\n",
        "                    return f\"Paths with slack less than {threshold}ns: {count} out of {len(all_slack_data)}\"\n",
        "\n",
        "            elif \"greater than\" in question_lower or \"above\" in question_lower or \"more than\" in question_lower or \">\" in question_lower:\n",
        "                # Extract threshold value\n",
        "                threshold = self._extract_threshold_from_question(question, \"greater\")\n",
        "                if threshold is not None:\n",
        "                    count = sum(1 for path in all_slack_data if path['slack'] > threshold)\n",
        "                    return f\"Paths with slack greater than {threshold}ns: {count} out of {len(all_slack_data)}\"\n",
        "\n",
        "            elif \"equal to\" in question_lower or \"==\" in question_lower or \"=\" in question_lower:\n",
        "                # Extract threshold value\n",
        "                threshold = self._extract_threshold_from_question(question, \"equal\")\n",
        "                if threshold is not None:\n",
        "                    count = sum(1 for path in all_slack_data if path['slack'] == threshold)\n",
        "                    return f\"Paths with slack equal to {threshold}ns: {count} out of {len(all_slack_data)}\"\n",
        "\n",
        "            elif \"between\" in question_lower:\n",
        "                # Extract range values\n",
        "                min_val, max_val = self._extract_range_from_question(question)\n",
        "                if min_val is not None and max_val is not None:\n",
        "                    count = sum(1 for path in all_slack_data if min_val <= path['slack'] <= max_val)\n",
        "                    return f\"Paths with slack between {min_val}ns and {max_val}ns: {count} out of {len(all_slack_data)}\"\n",
        "\n",
        "        # Default to total count\n",
        "        total_paths = len(all_slack_data)\n",
        "        return f\"Total number of timing paths: {total_paths}\"\n",
        "\n",
        "    def _handle_statistics_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle statistics queries using fast direct processing\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data available.\"\n",
        "\n",
        "        print(f\"DEBUG: Using direct statistics (no LLM)...\")\n",
        "        slack_values = [path['slack'] for path in all_slack_data]\n",
        "        avg_slack = sum(slack_values) / len(slack_values)\n",
        "        min_slack = min(slack_values)\n",
        "        max_slack = max(slack_values)\n",
        "\n",
        "        return f\"Slack statistics:\\n- Average: {avg_slack:.3f}ns\\n- Minimum: {min_slack:.3f}ns\\n- Maximum: {max_slack:.3f}ns\\n- Total paths: {len(slack_values)}\"\n",
        "\n",
        "    def _handle_filtering_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle filtering queries using fast direct processing\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data available.\"\n",
        "\n",
        "        print(f\"DEBUG: Using direct filtering (no LLM)...\")\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Handle technical reasoning queries\n",
        "        if any(word in question_lower for word in [\"can slacks\", \"if clock\", \"relationship\", \"how does\", \"positive clock\", \"negative clock\", \"skew effect\"]):\n",
        "            return self._generate_technical_reasoning(all_slack_data, question)\n",
        "\n",
        "        # Handle recommendation queries with comprehensive advice\n",
        "        if any(word in question_lower for word in [\"recommend\", \"suggest\", \"optimize\", \"improve\", \"how can we\", \"what can we do\", \"make better\", \"better slack\"]):\n",
        "            return self._generate_slack_improvement_recommendations(all_slack_data, question)\n",
        "\n",
        "        # Check for timing status queries first\n",
        "        if any(word in question_lower for word in [\"failing\", \"violation\", \"fail\", \"failed\", \"passing\", \"pass\", \"critical\", \"borderline\"]):\n",
        "            failing_paths = []\n",
        "            passing_paths = []\n",
        "\n",
        "            for path in all_slack_data:\n",
        "                slack = path['slack']\n",
        "                if slack < 0:\n",
        "                    failing_paths.append(path)\n",
        "                else:\n",
        "                    passing_paths.append(path)\n",
        "\n",
        "            results = []\n",
        "            if any(word in question_lower for word in [\"failing\", \"violation\", \"fail\", \"failed\"]):\n",
        "                if failing_paths:\n",
        "                    results.append(\"🔴 FAILING PATHS (Negative Slack = Timing Violation):\")\n",
        "                    for path in failing_paths:\n",
        "                        results.append(f\"  📍 {path['startpoint']} → {path['endpoint']}: Slack={path['slack']:.3f}ns [VIOLATION]\")\n",
        "                else:\n",
        "                    results.append(\"✅ GOOD NEWS: NO FAILING PATHS!\")\n",
        "                    results.append(\"All paths have positive slack values (timing passes)\")\n",
        "                    results.append(f\"📊 Summary: {len(passing_paths)} paths PASS timing\")\n",
        "\n",
        "            elif any(word in question_lower for word in [\"passing\", \"pass\", \"critica\"]):\n",
        "                critical_count = sum(1 for p in passing_paths if 0 <= p['slack'] < 0.1)\n",
        "                results.append(f\"🟢 TIMING STATUS SUMMARY:\")\n",
        "                results.append(f\"  📈 PASSING PATHS: {len(passing_paths)} paths\")\n",
        "                results.append(f\"  📈 FAILING PATHS: {len(failing_paths)} paths\")\n",
        "                results.append(f\"  ⚠️ CRITICAL PATHS: {critical_count} paths with <0.1ns margin\")\n",
        "\n",
        "            return \"\\n\".join(results) if results else \"No timing status data available\"\n",
        "\n",
        "        # Check if asking for clock skew specifically\n",
        "        if \"clock skew\" in question_lower or \"skew\" in question_lower:\n",
        "            result = \"Clock skew information for all paths:\\n\"\n",
        "            for i, path in enumerate(all_slack_data, 1):\n",
        "                clock_skew = path.get('clock_skew', 'N/A')\n",
        "                if clock_skew != 'N/A':\n",
        "                    result += f\"{i}. Path {path['startpoint']} to {path['endpoint']} - Clock skew: {clock_skew}ns - Slack: {path['slack']}ns\\n\"\n",
        "                else:\n",
        "                    result += f\"{i}. Path {path['startpoint']} to {path['endpoint']} - Clock skew: Not available - Slack: {path['slack']}ns\\n\"\n",
        "            return result.strip()\n",
        "\n",
        "        # Check if asking for hold time requirements specifically\n",
        "        elif \"hold time\" in question_lower:\n",
        "            result = \"Hold time requirements for all paths:\\n\"\n",
        "            for i, path in enumerate(all_slack_data, 1):\n",
        "                hold_time = path.get('hold_time_requirement', 'N/A')\n",
        "                if hold_time != 'N/A':\n",
        "                    result += f\"{i}. Path {path['startpoint']} to {path['endpoint']} - Hold time requirement: {hold_time}ns - Slack: {path['slack']}ns\\n\"\n",
        "                else:\n",
        "                    result += f\"{i}. Path {path['startpoint']} to {path['endpoint']} - Hold time requirement: Not available - Slack: {path['slack']}ns\\n\"\n",
        "            return result.strip()\n",
        "\n",
        "        # Check if asking for startpoints/endpoints specifically\n",
        "        elif \"startpoint\" in question_lower and \"endpoint\" in question_lower:\n",
        "            result = \"Startpoints and endpoints for all paths:\\n\"\n",
        "            for i, path in enumerate(all_slack_data, 1):\n",
        "                result += f\"{i}. Startpoint: {path['startpoint']} - Endpoint: {path['endpoint']} - Slack: {path['slack']}ns\\n\"\n",
        "            return result.strip()\n",
        "\n",
        "        else:\n",
        "            # Generic filtering - show all paths with all details\n",
        "            result = \"All timing paths with details:\\n\"\n",
        "            for i, path in enumerate(all_slack_data, 1):\n",
        "                result += f\"{i}. Slack: {path['slack']}ns - Start: {path['startpoint']} - End: {path['endpoint']} - Group: {path['group']}\\n\"\n",
        "                if path.get('clock_skew') != 'N/A':\n",
        "                    result += f\"   Clock skew: {path['clock_skew']}ns\\n\"\n",
        "                if path.get('hold_time_requirement') != 'N/A':\n",
        "                    result += f\"   Hold time requirement: {path['hold_time_requirement']}ns\\n\"\n",
        "            return result.strip()\n",
        "\n",
        "    def _handle_navigation_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle navigation queries (next path, etc.)\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data available.\"\n",
        "\n",
        "        # Sort paths by slack (worst to best)\n",
        "        sorted_paths = sorted(all_slack_data, key=lambda x: x['slack'])\n",
        "\n",
        "        # Get the next path after current index\n",
        "        if self.current_path_index < len(sorted_paths):\n",
        "            next_path = sorted_paths[self.current_path_index]\n",
        "            self.current_path_index += 1\n",
        "            return f\"Next path slack: {next_path['slack']}ns for the path from {next_path['startpoint']} to {next_path['endpoint']} in group {next_path['group']}.\"\n",
        "        else:\n",
        "            return \"No more paths available. All paths have been shown.\"\n",
        "\n",
        "    def _extract_number_from_question(self, question: str) -> int:\n",
        "        \"\"\"Extract number from question (e.g., 'top 2 worst' -> 2)\"\"\"\n",
        "        import re\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Look for patterns like \"top 2\", \"worst 3\", \"2 worst\", etc.\n",
        "        patterns = [\n",
        "            r'top\\s+(\\d+)',\n",
        "            r'worst\\s+(\\d+)',\n",
        "            r'(\\d+)\\s+worst',\n",
        "            r'(\\d+)\\s+best',\n",
        "            r'best\\s+(\\d+)',\n",
        "            r'(\\d+)\\s+paths?'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, question_lower)\n",
        "            if match:\n",
        "                return int(match.group(1))\n",
        "\n",
        "        # Default to 2 if no number found\n",
        "        return 2\n",
        "\n",
        "    def _extract_threshold_from_question(self, question: str, condition_type: str) -> float:\n",
        "        \"\"\"Extract numerical threshold from questions like 'slack less than 1ns'\"\"\"\n",
        "        import re\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Look for patterns with numbers followed by 'ns'\n",
        "        patterns = [\n",
        "            r'(\\d+\\.?\\d*)\\s*ns',  # \"1ns\", \"1.5ns\", etc.\n",
        "            r'(\\d+\\.?\\d*)',       # Just numbers\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, question_lower)\n",
        "            if matches:\n",
        "                try:\n",
        "                    value = float(matches[-1])  # Take the last number found\n",
        "                    return value\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_range_from_question(self, question: str) -> tuple:\n",
        "        \"\"\"Extract range values from questions like 'slack between 0.5ns and 1ns'\"\"\"\n",
        "        import re\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Look for \"between X and Y\" patterns\n",
        "        pattern = r'between\\s+(\\d+\\.?\\d*)\\s+ns?\\s+and\\s+(\\d+\\.?\\d*)\\s+ns?'\n",
        "        match = re.search(pattern, question_lower)\n",
        "        if match:\n",
        "            try:\n",
        "                min_val = float(match.group(1))\n",
        "                max_val = float(match.group(2))\n",
        "                return min_val, max_val\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "\n",
        "        # Fallback: extract all numbers\n",
        "        numbers = re.findall(r'(\\d+\\.?\\d*)', question_lower)\n",
        "        if len(numbers) >= 2:\n",
        "            try:\n",
        "                min_val = float(numbers[0])\n",
        "                max_val = float(numbers[1])\n",
        "                return min_val, max_val\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def _create_focused_context(self, all_slack_data: List[Dict], question: str) -> str:\n",
        "        \"\"\"Create focused context to avoid overwhelming LLM\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available.\"\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Focus on relevant data based on question\n",
        "        if \"worst\" in question_lower or \"bad\" in question_lower:\n",
        "            worst_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "            return f\"Worst path: {worst_path['slack']}ns slack, {worst_path['startpoint']}→{worst_path['endpoint']}, clock_skew:{worst_path.get('clock_skew', 'N/A')}, hold_time:{worst_path.get('hold_time_requirement', 'N/A')}\"\n",
        "\n",
        "        elif \"best\" in question_lower or \"good\" in question_lower:\n",
        "            best_path = max(all_slack_data, key=lambda x: x['slack'])\n",
        "            return f\"Best path: {best_path['slack']}ns slack, {best_path['startpoint']}→{best_path['endpoint']}\"\n",
        "\n",
        "        else:\n",
        "            # General summary\n",
        "            slack_values = [p['slack'] for p in all_slack_data]\n",
        "            return f\"{len(all_slack_data)} paths: slack range {min(slack_values):.3f} to {max(slack_values):.3f}ns, all positive (pass timing)\"\n",
        "\n",
        "    def _generate_llm_response(self, question: str, context: str) -> str:\n",
        "        \"\"\"Generate response using CodeLlama with proper attention mask and timeout\"\"\"\n",
        "        if not self.llm_model or not self.tokenizer:\n",
        "            return \"LLM not available for response generation.\"\n",
        "\n",
        "        # Create focused context to avoid overwhelming the model\n",
        "        summary_context = self._create_focused_context(self._get_all_slack_data(), question)\n",
        "\n",
        "        prompt = f\"\"\"<s>[INST] Timing Analysis Question:\n",
        "\n",
        "Data: {summary_context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer briefly (2-4 lines): [/INST]\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"DEBUG: Tokenizing prompt...\")\n",
        "            inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            print(f\"DEBUG: Input shape: {inputs.shape}\")\n",
        "\n",
        "            # Create attention mask\n",
        "            attention_mask = (inputs != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "            print(\"DEBUG: Starting LLM generation...\")\n",
        "            import threading\n",
        "            import time\n",
        "\n",
        "            result = [None]\n",
        "            exception = [None]\n",
        "\n",
        "            def generate_worker():\n",
        "                try:\n",
        "                    with torch.no_grad():\n",
        "                        outputs = self.llm_model.generate(\n",
        "                            inputs,\n",
        "                            attention_mask=attention_mask,\n",
        "                            max_new_tokens=50,  # Reduced for faster generation\n",
        "                            num_return_sequences=1,\n",
        "                            temperature=0.3,  # Lower temp for consistency\n",
        "                            do_sample=False,   # Deterministic\n",
        "                            pad_token_id=self.tokenizer.eos_token_id,\n",
        "                            eos_token_id=self.tokenizer.eos_token_id,\n",
        "                            early_stopping=True\n",
        "                        )\n",
        "                        result[0] = outputs\n",
        "                except Exception as e:\n",
        "                    exception[0] = e\n",
        "\n",
        "            # Start generation in a thread\n",
        "            thread = threading.Thread(target=generate_worker)\n",
        "            thread.start()\n",
        "            thread.join(timeout=15)  # 15 second timeout\n",
        "\n",
        "            if thread.is_alive():\n",
        "                print(\"DEBUG: LLM generation timed out after 15s, killing thread and using fallback...\")\n",
        "                # Thread is still alive, can't kill it cleanly, but let it continue in background\n",
        "                return self._generate_fallback_answer(question, None)\n",
        "\n",
        "            if exception[0]:\n",
        "                print(f\"DEBUG: LLM generation failed: {exception[0]}, using fallback...\")\n",
        "                return self._generate_fallback_answer(question, None)\n",
        "\n",
        "            if result[0] is None:\n",
        "                print(\"DEBUG: No result from LLM, using fallback...\")\n",
        "                return self._generate_fallback_answer(question, None)\n",
        "\n",
        "            outputs = result[0]\n",
        "\n",
        "            print(\"DEBUG: LLM generation completed, decoding...\")\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            answer = response.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "            print(f\"DEBUG: Response decoded, length: {len(answer)}\")\n",
        "\n",
        "            # If answer is too short or seems incomplete, use fallback\n",
        "            if len(answer.strip()) < 10:\n",
        "                print(\"DEBUG: LLM response too short, using fallback...\")\n",
        "                return self._generate_fallback_answer(question, None)\n",
        "\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in LLM response generation: {e}\")\n",
        "            return self._generate_fallback_answer(question, None)\n",
        "\n",
        "    def _generate_fallback_answer(self, question: str, context_or_data) -> str:\n",
        "        \"\"\"Generate fallback answer when LLM fails\"\"\"\n",
        "        print(\"DEBUG: Generating fallback answer...\")\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Always get fresh data for fallback\n",
        "        all_slack_data = self._get_all_slack_data()\n",
        "\n",
        "        # Handle specific question types with direct logic\n",
        "        if \"worst slack\" in question_lower or \"reason\" in question_lower:\n",
        "            if all_slack_data:\n",
        "                worst_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "                clock_skew = worst_path.get('clock_skew', 'N/A')\n",
        "                hold_req = worst_path.get('hold_time_requirement', 'N/A')\n",
        "\n",
        "                reason_parts = []\n",
        "                if clock_skew != 'N/A':\n",
        "                    try:\n",
        "                        if float(clock_skew) > 0.4:\n",
        "                            reason_parts.append(f\"High clock skew ({clock_skew}ns)\")\n",
        "                    except:\n",
        "                        pass\n",
        "                if hold_req != 'N/A':\n",
        "                    try:\n",
        "                        if float(hold_req) > 0.1:\n",
        "                            reason_parts.append(f\"High hold time requirement ({hold_req}ns)\")\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                if not reason_parts:\n",
        "                    reason_parts.append(\"Slack is positive but small margin\")\n",
        "\n",
        "                return f\"\"\"Worst slack: {worst_path['slack']:.3f}ns for {worst_path['startpoint']} → {worst_path['endpoint']}\n",
        "\n",
        "Potential reasons:\n",
        "{chr(10).join('- ' + reason for reason in reason_parts)}\n",
        "\n",
        "Note: This path still passes timing (positive slack) but has the smallest margin.\"\"\"\n",
        "\n",
        "        elif \"reason\" in question_lower or \"why\" in question_lower:\n",
        "            if all_slack_data:\n",
        "                return f\"\"\"Timing Analysis Summary:\n",
        "- Total paths: {len(all_slack_data)}\n",
        "- All paths PASS timing (positive slack)\n",
        "- Slack range: {min(p['slack'] for p in all_slack_data):.3f}ns to {max(p['slack'] for p in all_slack_data):.3f}ns\n",
        "\n",
        "Potential timing concerns:\n",
        "- Small slack margins (both < 1ns)\n",
        "- Clock skew and hold time requirements may impact design margin\"\"\"\n",
        "\n",
        "        else:\n",
        "            return f\"Analysis unavailable due to LLM timeout. Raw data: {len(all_slack_data)} paths processed.\"\n",
        "\n",
        "    def query(self, question: str, top_k: int = 3) -> str:\n",
        "        \"\"\"Query the RAG system\"\"\"\n",
        "        import time\n",
        "        query_start = time.time()\n",
        "\n",
        "        print(f\"DEBUG: Query method called with: '{question}'\")\n",
        "\n",
        "        try:\n",
        "            # Add to history\n",
        "            print(\"DEBUG: Adding to history...\")\n",
        "            self.history.append({'question': question, 'answer': ''})\n",
        "\n",
        "            # Get all slack data for comprehensive analysis\n",
        "            print(\"DEBUG: Getting all slack data...\")\n",
        "            all_slack_data = self._get_all_slack_data()\n",
        "            print(f\"DEBUG: Retrieved {len(all_slack_data)} slack data entries\")\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Error in initial query setup: {e}\")\n",
        "            return f\"Error in query setup: {e}\"\n",
        "\n",
        "        # Classify the question\n",
        "        classify_start = time.time()\n",
        "        print(\"DEBUG: Classifying question...\")\n",
        "        question_type = self._classify_question(question, all_slack_data)\n",
        "        classify_time = time.time() - classify_start\n",
        "        print(f\"DEBUG: Question classified as: '{question_type}' (took {classify_time:.2f}s)\")\n",
        "\n",
        "        # Generate response based on high-level question type\n",
        "        handler_start = time.time()\n",
        "        print(f\"DEBUG: Routing to handler for type: {question_type}\")\n",
        "\n",
        "        if question_type == \"complex\":\n",
        "            print(\"DEBUG: Calling complex query handler\")\n",
        "            result = self._handle_complex_query(question, all_slack_data)\n",
        "        elif question_type == \"navigation\":\n",
        "            print(\"DEBUG: Calling navigation handler\")\n",
        "            result = self._handle_navigation_query(question, all_slack_data)\n",
        "        elif question_type == \"ranking\":\n",
        "            print(\"DEBUG: Calling ranking handler\")\n",
        "            result = self._handle_ranking_query(question, all_slack_data)\n",
        "        elif question_type == \"counting\":\n",
        "            print(\"DEBUG: Calling counting handler\")\n",
        "            result = self._handle_counting_query(question, all_slack_data)\n",
        "        elif question_type == \"statistics\":\n",
        "            print(\"DEBUG: Calling statistics handler\")\n",
        "            result = self._handle_statistics_query(question, all_slack_data)\n",
        "        elif question_type == \"filtering\":\n",
        "            print(\"DEBUG: Calling filtering handler\")\n",
        "            result = self._handle_filtering_query(question, all_slack_data)\n",
        "\n",
        "        # Add final timing\n",
        "        handler_end = time.time()\n",
        "        query_end = time.time()\n",
        "        handler_time = handler_end - handler_start\n",
        "        total_time = query_end - query_start\n",
        "        print(f\"DEBUG: Handler completed in {handler_time:.2f}s, total query time: {total_time:.2f}s\")\n",
        "\n",
        "        # Add else clause for unmatched types\n",
        "        if question_type not in [\"ranking\", \"counting\", \"statistics\", \"filtering\", \"navigation\", \"complex\"]:\n",
        "            result = f\"I don't understand the question type. Please ask about rankings (worst/best paths), counts (how many paths), statistics (slack analysis), or filtering (show paths).\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _handle_complex_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle complex queries using LLM with structured context\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available for analysis.\"\n",
        "\n",
        "        print(\"DEBUG: Using LLM for complex query analysis...\")\n",
        "\n",
        "        # Prepare comprehensive data context for LLM\n",
        "        data_context = self._prepare_comprehensive_context(all_slack_data)\n",
        "\n",
        "        # Use the LLM for complex analysis\n",
        "        return self._generate_llm_response(question, data_context)\n",
        "\n",
        "    def _prepare_comprehensive_context(self, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Prepare comprehensive timing data context for LLM analysis\"\"\"\n",
        "        context = \"TIMING ANALYSIS DATA:\\n\"\n",
        "        context += f\"Total paths analyzed: {len(all_slack_data)}\\n\\n\"\n",
        "\n",
        "        # Add summary statistics\n",
        "        slack_values = [path['slack'] for path in all_slack_data]\n",
        "        context += f\"SLACK SUMMARY:\\n\"\n",
        "        context += f\"- Minimum slack: {min(slack_values):.3f}ns\\n\"\n",
        "        context += f\"- Maximum slack: {max(slack_values):.3f}ns\\n\"\n",
        "        context += f\"- Average slack: {sum(slack_values)/len(slack_values):.3f}ns\\n\\n\"\n",
        "\n",
        "        # Add individual path details\n",
        "        context += f\"DETAILED PATH ANALYSIS:\\n\"\n",
        "        for i, path in enumerate(all_slack_data, 1):\n",
        "            context += f\"Path {i}:\\n\"\n",
        "            context += f\"  Startpoint: {path['startpoint']}\\n\"\n",
        "            context += f\"  Endpoint: {path['endpoint']}\\n\"\n",
        "            context += f\"  Slack: {path['slack']}ns\\n\"\n",
        "            context += f\"  Group: {path['group']}\\n\"\n",
        "\n",
        "            # Add clock skew if available\n",
        "            if path.get('clock_skew'):\n",
        "                clock_skew = path['clock_skew']\n",
        "                context += f\"  Clock skew: {clock_skew}ns\\n\"\n",
        "\n",
        "                # Add interpretation\n",
        "                current_slack = slack_values[i-1]\n",
        "                if current_slack < 0:\n",
        "                    context += f\"  Slack analysis: TIMING FAILURE (negative slack = violation)\\n\"\n",
        "                elif current_slack < 0.1:\n",
        "                    context += f\"  Slack analysis: CRITICAL (very small positive margin)\\n\"\n",
        "                else:\n",
        "                    context += f\"  Slack analysis: TIMING PASS (adequate positive margin)\\n\"\n",
        "\n",
        "                if clock_skew > 0.4:\n",
        "                    context += f\"  Clock skew analysis: HIGH (may impact timing margin)\\n\"\n",
        "\n",
        "            # Add hold time if available\n",
        "            if path.get('hold_time_requirement'):\n",
        "                hold_time = path['hold_time_requirement']\n",
        "                context += f\"  Hold time requirement: {hold_time}ns\\n\"\n",
        "\n",
        "            context += \"\\n\"\n",
        "\n",
        "        # Add analysis guidance\n",
        "        context += \"ANALYSIS GUIDANCE:\\n\"\n",
        "        context += \"- POSITIVE slack = timing PASS (data arrives before required time)\\n\"\n",
        "        context += \"- NEGATIVE slack = timing FAILURE (violation)\\n\"\n",
        "        context += \"- Slack < 0.1ns = critical margin (close to violation)\\n\"\n",
        "        context += \"- Clock skew > 0.4ns typically impacts timing margin\\n\"\n",
        "        context += \"- Current data shows ONLY positive slacks (all paths PASS)\\n\"\n",
        "\n",
        "        return context\n",
        "\n",
        "def main():\n",
        "    # Create RAG system\n",
        "    rag = ImprovedLocalTimingRAG()\n",
        "\n",
        "    # Index timing reports\n",
        "    rag.index_timing_reports('./timing_reports/')\n",
        "\n",
        "    # Interactive query loop\n",
        "    print(\"\\n=== Improved Timing RAG System Ready ===\")\n",
        "    print(\"Ask questions about your timing data. Type 'quit' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nQuestion: \").strip()\n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            break\n",
        "\n",
        "        if question:\n",
        "            answer = rag.query(question)\n",
        "            print(f\"Answer: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import json\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from typing import List, Dict, Any, Optional\n",
        "import shutil\n",
        "import logging\n",
        "\n",
        "# Suppress ChromaDB telemetry errors\n",
        "logging.getLogger(\"chromadb.telemetry.product.posthog\").setLevel(logging.ERROR)\n",
        "\n",
        "class ImprovedLocalTimingRAG:\n",
        "    def __init__(self, model_name: str = \"codellama/CodeLlama-7b-Instruct-hf\"):\n",
        "        self.model_name = model_name\n",
        "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.llm_model, self.tokenizer = self._load_local_llm()\n",
        "\n",
        "        # Get device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Use in-memory database to avoid file permission issues\n",
        "        self._setup_database()\n",
        "\n",
        "        self.history = []\n",
        "        self.current_path_index = 0  # Track current path for \"next\" queries\n",
        "\n",
        "    def _setup_database(self):\n",
        "        \"\"\"Setup ChromaDB with in-memory storage\"\"\"\n",
        "        try:\n",
        "            # Use in-memory database to avoid file permission issues\n",
        "            self.client = chromadb.Client()\n",
        "            # Try to get existing collection, if it exists, delete it first\n",
        "            try:\n",
        "                existing_collection = self.client.get_collection(name=\"timing_reports\")\n",
        "                self.client.delete_collection(name=\"timing_reports\")\n",
        "            except:\n",
        "                pass  # Collection doesn't exist, which is fine\n",
        "\n",
        "            self.collection = self.client.create_collection(name=\"timing_reports\")\n",
        "            print(\"Using in-memory database\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up database: {e}\")\n",
        "            # Fallback to persistent client with a different path\n",
        "            try:\n",
        "                # Clear existing database\n",
        "                if os.path.exists(\"./temp_chroma_db\"):\n",
        "                    shutil.rmtree(\"./temp_chroma_db\")\n",
        "\n",
        "                self.client = chromadb.PersistentClient(path=\"./temp_chroma_db\")\n",
        "                self.collection = self.client.create_collection(name=\"timing_reports\")\n",
        "                print(\"Using persistent database at ./temp_chroma_db\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Error with persistent database: {e2}\")\n",
        "                # Last resort - create a new in-memory client\n",
        "                self.client = chromadb.Client()\n",
        "                try:\n",
        "                    existing_collection = self.client.get_collection(name=\"timing_reports\")\n",
        "                    self.client.delete_collection(name=\"timing_reports\")\n",
        "                except:\n",
        "                    pass\n",
        "                self.collection = self.client.create_collection(name=\"timing_reports\")\n",
        "                print(\"Using fallback in-memory database\")\n",
        "\n",
        "    def _load_local_llm(self):\n",
        "        \"\"\"Load the local LLM and tokenizer with proper attention mask handling\"\"\"\n",
        "        try:\n",
        "            print(f\"Loading {self.model_name}...\")\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "\n",
        "            # Set pad token if not set\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "            # Set padding side to left for better generation\n",
        "            tokenizer.padding_side = \"left\"\n",
        "\n",
        "            print(f\"Successfully loaded {self.model_name}\")\n",
        "            return model, tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model {self.model_name}: {e}\")\n",
        "            print(\"Falling back to DialoGPT-large...\")\n",
        "            try:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "                model = AutoModelForCausalLM.from_pretrained(\n",
        "                    \"microsoft/DialoGPT-large\",\n",
        "                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "                    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "                )\n",
        "                if tokenizer.pad_token is None:\n",
        "                    tokenizer.pad_token = tokenizer.eos_token\n",
        "                tokenizer.padding_side = \"left\"\n",
        "                return model, tokenizer\n",
        "            except Exception as e2:\n",
        "                print(f\"Error loading fallback model: {e2}\")\n",
        "                return None, None\n",
        "\n",
        "    def _read_json_file(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Read JSON file with aggressive repair for malformed JSON\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Try normal parsing first\n",
        "            try:\n",
        "                return json.loads(content)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"JSON parsing failed: {e}\")\n",
        "                print(\"Attempting aggressive JSON repair...\")\n",
        "\n",
        "                # Aggressive JSON repair\n",
        "                repaired_content = self._repair_json(content)\n",
        "                return json.loads(repaired_content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"File reading failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _repair_json(self, content: str) -> str:\n",
        "        \"\"\"Aggressively repair malformed JSON\"\"\"\n",
        "        import re\n",
        "\n",
        "        # Fix 1: Remove trailing commas\n",
        "        content = re.sub(r',\\s*}', '}', content)\n",
        "        content = re.sub(r',\\s*]', ']', content)\n",
        "\n",
        "        # Fix 2: Add missing commas between objects/arrays\n",
        "        content = re.sub(r'}\\s*{', '}, {', content)\n",
        "        content = re.sub(r']\\s*\\[', '], [', content)\n",
        "        content = re.sub(r'}\\s*\\[', '}, [', content)\n",
        "        content = re.sub(r']\\s*{', '], {', content)\n",
        "\n",
        "        # Fix 3: Handle specific line 34558 issue - look for missing comma patterns\n",
        "        lines = content.split('\\n')\n",
        "        if len(lines) > 34557:\n",
        "            # Check the problematic line and surrounding context\n",
        "            problem_line = lines[34557]  # 0-indexed\n",
        "            print(f\"Problem line 34558: {repr(problem_line)}\")\n",
        "\n",
        "            # Try to fix common patterns on this line\n",
        "            if problem_line.strip().endswith('}') and not problem_line.strip().endswith(',}'):\n",
        "                # Look at the next line to see if it starts with {\n",
        "                if len(lines) > 34558 and lines[34558].strip().startswith('{'):\n",
        "                    lines[34557] = problem_line.rstrip() + ','\n",
        "                    content = '\\n'.join(lines)\n",
        "                    print(\"Fixed missing comma at line 34558\")\n",
        "\n",
        "        # Fix 4: More aggressive comma insertion\n",
        "        # Look for patterns like: } followed by { on next line\n",
        "        content = re.sub(r'}\\s*\\n\\s*{', '},\\n{', content)\n",
        "        content = re.sub(r']\\s*\\n\\s*\\[', '],\\n[', content)\n",
        "\n",
        "        # Fix 5: Handle unclosed strings or other issues\n",
        "        # This is a last resort - try to balance braces and brackets\n",
        "        open_braces = content.count('{') - content.count('}')\n",
        "        open_brackets = content.count('[') - content.count(']')\n",
        "\n",
        "        if open_braces > 0:\n",
        "            content += '}' * open_braces\n",
        "            print(f\"Added {open_braces} closing braces\")\n",
        "        if open_brackets > 0:\n",
        "            content += ']' * open_brackets\n",
        "            print(f\"Added {open_brackets} closing brackets\")\n",
        "\n",
        "        return content\n",
        "\n",
        "    def index_timing_reports(self, directory: str):\n",
        "        \"\"\"Index all JSON files in the directory\"\"\"\n",
        "        print(\"=== Indexing timing reports ===\")\n",
        "\n",
        "        json_files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
        "        print(f\"Found {len(json_files)} JSON files to index\")\n",
        "\n",
        "        for filename in json_files:\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            print(f\"Indexing {filename}...\")\n",
        "\n",
        "            try:\n",
        "                data = self._read_json_file(file_path)\n",
        "                if not data:\n",
        "                    print(f\"Failed to read {filename}\")\n",
        "                    continue\n",
        "\n",
        "                # Extract text and metadata for each path\n",
        "                for i, path in enumerate(data.get('paths', [])):\n",
        "                    # Create a comprehensive text representation\n",
        "                    text_parts = []\n",
        "\n",
        "                    # Add path metadata\n",
        "                    if 'startpoint' in path:\n",
        "                        text_parts.append(f\"Startpoint: {path['startpoint'].get('instance', 'N/A')}\")\n",
        "                    if 'endpoint' in path:\n",
        "                        text_parts.append(f\"Endpoint: {path['endpoint'].get('instance', 'N/A')}\")\n",
        "                    if 'report' in path:\n",
        "                        report = path['report']\n",
        "                        text_parts.append(f\"Group: {report.get('group', 'N/A')}\")\n",
        "                        text_parts.append(f\"Path Type: {report.get('path_type', 'N/A')}\")\n",
        "\n",
        "                    # Add summary information\n",
        "                    if 'summary' in path:\n",
        "                        summary = path['summary']\n",
        "                        if 'slack' in summary:\n",
        "                            text_parts.append(f\"Slack: {summary['slack']}\")\n",
        "                        if 'hold_time_requirement' in summary and summary['hold_time_requirement'] is not None:\n",
        "                            text_parts.append(f\"Hold Time Requirement: {summary['hold_time_requirement']}\")\n",
        "                        if 'clock_skew' in summary and summary['clock_skew'] is not None:\n",
        "                            text_parts.append(f\"Clock Skew: {summary['clock_skew']}\")\n",
        "\n",
        "                    # Add launch clock path stages\n",
        "                    if 'launch_clock_path' in path and 'stages' in path['launch_clock_path']:\n",
        "                        text_parts.append(\"Launch Clock Path:\")\n",
        "                        for stage in path['launch_clock_path']['stages']:\n",
        "                            if stage.get('type') == 'cell_pin':\n",
        "                                text_parts.append(f\"  {stage.get('name', 'N/A')} ({stage.get('cell_type', 'N/A')})\")\n",
        "\n",
        "                    # Add data path stages\n",
        "                    if 'data_path' in path and 'stages' in path['data_path']:\n",
        "                        text_parts.append(\"Data Path:\")\n",
        "                        for stage in path['data_path']['stages']:\n",
        "                            if stage.get('type') == 'cell_pin':\n",
        "                                text_parts.append(f\"  {stage.get('name', 'N/A')} ({stage.get('cell_type', 'N/A')})\")\n",
        "\n",
        "                    # Add capture clock path stages\n",
        "                    if 'capture_clock_path' in path and 'stages' in path['capture_clock_path']:\n",
        "                        text_parts.append(\"Capture Clock Path:\")\n",
        "                        for stage in path['capture_clock_path']['stages']:\n",
        "                            if stage.get('type') == 'cell_pin':\n",
        "                                text_parts.append(f\"  {stage.get('name', 'N/A')} ({stage.get('cell_type', 'N/A')})\")\n",
        "\n",
        "                    # Combine all text\n",
        "                    full_text = \"\\n\".join(text_parts)\n",
        "\n",
        "                    # Generate embedding\n",
        "                    embedding = self.embedding_model.encode(full_text).tolist()\n",
        "\n",
        "                    # Store in ChromaDB\n",
        "                    self.collection.add(\n",
        "                        embeddings=[embedding],\n",
        "                        documents=[full_text],\n",
        "                        metadatas=[{\n",
        "                            'filename': filename,\n",
        "                            'path_index': i,\n",
        "                            'startpoint': path.get('startpoint', {}).get('instance', 'N/A'),\n",
        "                            'endpoint': path.get('endpoint', {}).get('instance', 'N/A'),\n",
        "                            'slack': path.get('summary', {}).get('slack', 'N/A'),\n",
        "                            'hold_time_requirement': path.get('summary', {}).get('hold_time_requirement', 'N/A'),\n",
        "                            'clock_skew': path.get('summary', {}).get('clock_skew', 'N/A'),\n",
        "                            'group': path.get('report', {}).get('group', 'N/A')\n",
        "                        }],\n",
        "                        ids=[f\"{filename}_{i}\"]\n",
        "                    )\n",
        "\n",
        "                print(f\"Successfully indexed {filename} with {len(data.get('paths', []))} paths\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error indexing {filename}: {e}\")\n",
        "\n",
        "        print(\"Indexing complete!\")\n",
        "\n",
        "    def _get_all_slack_data(self) -> List[Dict]:\n",
        "        \"\"\"Get all slack-related data from the collection\"\"\"\n",
        "        print(\"DEBUG: Getting all slack data from collection...\")\n",
        "        all_data = []\n",
        "\n",
        "        try:\n",
        "            print(\"DEBUG: Calling collection.get()...\")\n",
        "            results = self.collection.get()\n",
        "            print(f\"DEBUG: Collection.get() returned {len(results.get('metadatas', []))} entries\")\n",
        "\n",
        "            for i, metadata in enumerate(results['metadatas']):\n",
        "                if metadata.get('slack') != 'N/A':\n",
        "                    try:\n",
        "                        slack_value = float(metadata['slack'])\n",
        "                        all_data.append({\n",
        "                            'slack': slack_value,\n",
        "                            'startpoint': metadata.get('startpoint', 'N/A'),\n",
        "                            'endpoint': metadata.get('endpoint', 'N/A'),\n",
        "                            'group': metadata.get('group', 'N/A'),\n",
        "                            'hold_time_requirement': metadata.get('hold_time_requirement', 'N/A'),\n",
        "                            'clock_skew': metadata.get('clock_skew', 'N/A'),\n",
        "                            'document': results['documents'][i],\n",
        "                            'path_index': metadata.get('path_index', i)\n",
        "                        })\n",
        "                    except (ValueError, TypeError):\n",
        "                        continue\n",
        "\n",
        "            print(f\"DEBUG: Processed {len(all_data)} slack data entries\")\n",
        "            return all_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Error in _get_all_slack_data: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _get_clock_skew_data(self) -> List[Dict]:\n",
        "        \"\"\"Get all clock skew data from the collection\"\"\"\n",
        "        all_data = []\n",
        "        results = self.collection.get()\n",
        "\n",
        "        for i, metadata in enumerate(results['metadatas']):\n",
        "            if metadata.get('clock_skew') != 'N/A':\n",
        "                try:\n",
        "                    clock_skew_value = float(metadata['clock_skew'])\n",
        "                    all_data.append({\n",
        "                        'clock_skew': clock_skew_value,\n",
        "                        'startpoint': metadata.get('startpoint', 'N/A'),\n",
        "                        'endpoint': metadata.get('endpoint', 'N/A'),\n",
        "                        'group': metadata.get('group', 'N/A'),\n",
        "                        'slack': metadata.get('slack', 'N/A'),\n",
        "                        'document': results['documents'][i],\n",
        "                        'path_index': metadata.get('path_index', i)\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def _get_hold_time_data(self) -> List[Dict]:\n",
        "        \"\"\"Get all hold time requirement data from the collection\"\"\"\n",
        "        all_data = []\n",
        "        results = self.collection.get()\n",
        "\n",
        "        for i, metadata in enumerate(results['metadatas']):\n",
        "            if metadata.get('hold_time_requirement') != 'N/A':\n",
        "                try:\n",
        "                    hold_time_value = float(metadata['hold_time_requirement'])\n",
        "                    all_data.append({\n",
        "                        'hold_time_requirement': hold_time_value,\n",
        "                        'startpoint': metadata.get('startpoint', 'N/A'),\n",
        "                        'endpoint': metadata.get('endpoint', 'N/A'),\n",
        "                        'group': metadata.get('group', 'N/A'),\n",
        "                        'slack': metadata.get('slack', 'N/A'),\n",
        "                        'document': results['documents'][i],\n",
        "                        'path_index': metadata.get('path_index', i)\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        return all_data\n",
        "\n",
        "    def _classify_question_with_llm(self, question: str) -> str:\n",
        "        \"\"\"Use CodeLlama to classify the question type with timeout\"\"\"\n",
        "        import time\n",
        "\n",
        "        if not self.llm_model or not self.tokenizer:\n",
        "            print(\"DEBUG: LLM not available, returning 'general'\")\n",
        "            return \"general\"\n",
        "\n",
        "        print(f\"DEBUG: LLM classification for: '{question}'\")\n",
        "\n",
        "        # Much simpler prompt for faster classification\n",
        "        prompt = f\"\"\"<s>[INST] Classify this timing question:\n",
        "\n",
        "- ranking: worst/best/top questions\n",
        "- counting: how many/total questions\n",
        "- filtering: show/find questions\n",
        "- general: other questions\n",
        "\n",
        "Question: \"{question}\"\n",
        "\n",
        "Category: [/INST]\"\"\"\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            print(\"DEBUG: Tokenizing classification prompt...\")\n",
        "            inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)  # Shorter\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            print(\"DEBUG: Creating attention mask...\")\n",
        "            # Create attention mask\n",
        "            attention_mask = (inputs != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "            print(\"DEBUG: Starting LLM classification generation...\")\n",
        "            with torch.no_grad():\n",
        "                outputs = self.llm_model.generate(\n",
        "                    inputs,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_new_tokens=3,  # Very short for classification\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.1,\n",
        "                    do_sample=False,  # Deterministic for speed\n",
        "                    pad_token_id=self.tokenizer.eos_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            end_time = time.time()\n",
        "            print(f\"DEBUG: LLM generation took {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "            print(\"DEBUG: Decoding classification response...\")\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            category = response.split(\"[/INST]\")[-1].strip().lower()\n",
        "\n",
        "            print(f\"DEBUG: Raw LLM response: '{response}'\")\n",
        "            print(f\"DEBUG: Extracted category: '{category}'\")\n",
        "\n",
        "            # Clean up the response\n",
        "            category = category.replace(\"category:\", \"\").strip()\n",
        "\n",
        "            # Map to high-level categories\n",
        "            category = category.strip().lower()\n",
        "            if category in [\"ranking\", \"counting\", \"statistics\", \"filtering\", \"navigation\", \"general\"]:\n",
        "                print(f\"DEBUG: Valid category found: '{category}'\")\n",
        "                return category\n",
        "            else:\n",
        "                print(f\"DEBUG: Invalid category '{category}', returning 'general'\")\n",
        "                return \"general\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in LLM question classification: {e}\")\n",
        "            return \"general\"\n",
        "\n",
        "    def _detect_query_complexity(self, question: str) -> str:\n",
        "        \"\"\"Detect if question needs LLM processing or direct processing\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Simple data retrieval patterns\n",
        "        simple_patterns = [\n",
        "            \"worst\", \"best\", \"top\", \"bottom\", \"highest\", \"lowest\",  # Basic ranking\n",
        "            \"total\", \"count\", \"number of\",                           # Simple counts (NO conditions)\n",
        "            \"how many\",                                              # counting questions\n",
        "            \"paths are\", \"are there\", \"paths total\",                # Simple counting variations\n",
        "            \"average\", \"mean\", \"statistics\", \"distribution\",       # Basic statistics\n",
        "            \"clock skew\", \"skew\", \"hold time\",                     # Specific metrics\n",
        "            \"startpoint\", \"endpoint\", \"show\",                       # Data display\n",
        "            \"next\", \"previous\"                                      # Navigation\n",
        "        ]\n",
        "\n",
        "        # Simple timing status patterns (can be handled fast)\n",
        "        timing_status_patterns = [\n",
        "            \"failing\", \"violation\", \"fail\", \"failed\",            # Timing status\n",
        "            \"passing\", \"pass\", \"passed\",                          # Timing status\n",
        "            \"critical\", \"borderline\"                             # Timing status\n",
        "        ]\n",
        "\n",
        "        # Complex analytical patterns\n",
        "        complex_patterns = [\n",
        "            \"why\", \"why is\", \"why are\", \"reason\", \"cause\",         # Explanations\n",
        "            \"analyze\", \"analysis\", \"pattern\", \"correlation\",       # Analysis\n",
        "            \"compare\", \"difference\", \"between\",                    # Comparisons\n",
        "            \"what if\", \"impact\", \"effect\",                         # Causal analysis\n",
        "            \"less than\", \"greater than\", \"more than\", \"between\",   # Conditional queries\n",
        "            \"above\", \"below\", \"equal to\", \"higher than\", \"lower than\",  # Additional conditions\n",
        "            \"can slacks\", \"if clock\", \"relationship\", \"how does\",  # Technical reasoning\n",
        "            \"positive clock\", \"negative clock\", \"skew effect\"      # Clock skew analysis\n",
        "        ]\n",
        "\n",
        "        # Recommendation patterns (should handle with direct logic, not LLM)\n",
        "        recommendation_patterns = [\n",
        "            \"recommend\", \"suggest\", \"optimize\", \"improve\",\n",
        "            \"how can we\", \"what can we do\", \"how to improve\",\n",
        "            \"make better\", \"improve slack\", \"better slack\"\n",
        "        ]\n",
        "\n",
        "        # Check for simple timing status patterns first\n",
        "        for pattern in timing_status_patterns:\n",
        "            if pattern in question_lower:\n",
        "                return \"simple\"\n",
        "\n",
        "        # Check for recommendation patterns (handle with direct logic)\n",
        "        for pattern in recommendation_patterns:\n",
        "            if pattern in question_lower:\n",
        "                print(f\"DEBUG: Detected recommendation pattern '{pattern}', forcing simple classification\")\n",
        "                return \"simple\"\n",
        "\n",
        "        # Force simple classification for certain problematic phrases\n",
        "        problematic_phrases = [\n",
        "            \"reason for worst slack\",\n",
        "            \"why worst slack\",\n",
        "            \"worst slack reason\"\n",
        "        ]\n",
        "        for phrase in problematic_phrases:\n",
        "            if phrase in question_lower:\n",
        "                print(f\"DEBUG: Detected problematic phrase '{phrase}', forcing simple classification\")\n",
        "                return \"simple\"\n",
        "\n",
        "        # Check for complex indicators\n",
        "        for pattern in complex_patterns:\n",
        "            if pattern in question_lower:\n",
        "                return \"complex\"\n",
        "\n",
        "        # Check for simple indicators\n",
        "        for pattern in simple_patterns:\n",
        "            if pattern in question_lower:\n",
        "                return \"simple\"\n",
        "\n",
        "        return \"complex\"  # Default to complex for safety\n",
        "\n",
        "    def _classify_question(self, question: str, data: List[Dict]) -> str:\n",
        "        \"\"\"Smart classification: route between fast and LLM processing\"\"\"\n",
        "        print(f\"DEBUG: Classifying question: '{question}'\")\n",
        "\n",
        "        # Detect complexity first\n",
        "        complexity = self._detect_query_complexity(question)\n",
        "        print(f\"DEBUG: Query complexity: {complexity}\")\n",
        "\n",
        "        if complexity == \"simple\":\n",
        "            print(\"DEBUG: Using fast pattern matching for simple query...\")\n",
        "            question_lower = question.lower()\n",
        "\n",
        "            # Ranking patterns\n",
        "            if any(word in question_lower for word in [\"worst\", \"best\", \"top\", \"bottom\", \"highest\", \"lowest\", \"rank\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'ranking'\")\n",
        "                return \"ranking\"\n",
        "            # Counting patterns\n",
        "            elif any(word in question_lower for word in [\"how many\", \"total\", \"count\", \"number of\", \"are there\", \"paths are\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'counting'\")\n",
        "                return \"counting\"\n",
        "            # Statistics patterns\n",
        "            elif any(word in question_lower for word in [\"average\", \"mean\", \"median\", \"statistics\", \"distribution\", \"range\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'statistics'\")\n",
        "                return \"statistics\"\n",
        "            # Clock skew patterns\n",
        "            elif any(word in question_lower for word in [\"clock skew\", \"skew\", \"clock_skew\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'filtering' (clock skew)\")\n",
        "                return \"filtering\"\n",
        "            # Hold time patterns\n",
        "            elif any(word in question_lower for word in [\"hold time\", \"hold_time\", \"hold time requirement\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'filtering' (hold time)\")\n",
        "                return \"filtering\"\n",
        "            # Timing status patterns\n",
        "            elif any(word in question_lower for word in [\"failing\", \"violation\", \"crash\", \"crash\", \"passing\", \"pass\", \"critical\", \"borderline\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'filtering' (timing status)\")\n",
        "                return \"filtering\"\n",
        "            # Recommendation patterns\n",
        "            elif any(word in question_lower for word in [\"recommend\", \"suggest\", \"optimize\", \"improve\", \"how can we\", \"what can we do\", \"make better\", \"better slack\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'filtering' (recommendations)\")\n",
        "                return \"filtering\"\n",
        "            # Filtering patterns\n",
        "            elif any(word in question_lower for word in [\"startpoint\", \"endpoint\", \"show\", \"find\", \"search\", \"filter\", \"where\", \"which\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'filtering'\")\n",
        "                return \"filtering\"\n",
        "            # Navigation patterns\n",
        "            elif any(word in question_lower for word in [\"next\", \"previous\", \"browse\", \"iterate\", \"go to\"]):\n",
        "                print(\"DEBUG: Pattern matched as 'navigation'\")\n",
        "                return \"navigation\"\n",
        "            else:\n",
        "                print(\"DEBUG: Pattern matched as 'general'\")\n",
        "                return \"general\"\n",
        "        else:\n",
        "            # Complex query - return 'complex' category\n",
        "            print(f\"DEBUG: Complex query detected: '{question}', returning 'complex'\")\n",
        "            return \"complex\"\n",
        "\n",
        "    def _handle_ranking_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle ranking queries with metric detection\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data available.\"\n",
        "\n",
        "        print(f\"DEBUG: Handling ranking query: {question}\")\n",
        "        print(f\"DEBUG: Found {len(all_slack_data)} paths\")\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Check for specific metric requests\n",
        "        if \"clock skew\" in question_lower or \"skew\" in question_lower:\n",
        "            print(f\"DEBUG: Clock skew ranking requested\")\n",
        "            return self._handle_clock_skew_ranking(question, all_slack_data)\n",
        "        elif \"hold time\" in question_lower or \"hold_time\" in question_lower:\n",
        "            print(f\"DEBUG: Hold time ranking requested\")\n",
        "            return self._handle_hold_time_ranking(question, all_slack_data)\n",
        "\n",
        "        # Default to slack ranking\n",
        "        # Extract number from question for fallback\n",
        "        n = self._extract_number_from_question(question)\n",
        "        print(f\"DEBUG: Extracted number: {n}\")\n",
        "\n",
        "        # Skip LLM entirely - it's too slow!\n",
        "        print(f\"DEBUG: Using direct processing (no LLM - it's too slow!)\")\n",
        "        return self._handle_ranking_fallback(question, all_slack_data, n)\n",
        "\n",
        "    def _handle_ranking_fallback(self, question: str, all_slack_data: List[Dict], n: int) -> str:\n",
        "        \"\"\"Fallback ranking handler without LLM\"\"\"\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Check if asking for reasons/analysis specifically\n",
        "        if \"reason\" in question_lower or (\"why\" in question_lower and \"worst\" in question_lower):\n",
        "            return self._analyze_worst_slack_reasons(all_slack_data)\n",
        "\n",
        "        # Sort paths by slack\n",
        "        if \"worst\" in question_lower or \"bottom\" in question_lower:\n",
        "            sorted_paths = sorted(all_slack_data, key=lambda x: x['slack'])\n",
        "            direction = \"worst\"\n",
        "        else:\n",
        "            sorted_paths = sorted(all_slack_data, key=lambda x: x['slack'], reverse=True)\n",
        "            direction = \"best\"\n",
        "\n",
        "        # Get top N paths\n",
        "        n = min(n, len(all_slack_data))\n",
        "        top_paths = sorted_paths[:n]\n",
        "\n",
        "        result = f\"Top {n} {direction} slack paths:\\n\"\n",
        "        for i, path in enumerate(top_paths, 1):\n",
        "            result += f\"{i}. Slack: {path['slack']}ns - Path from {path['startpoint']} to {path['endpoint']} in group {path['group']}\\n\"\n",
        "\n",
        "        return result.strip()\n",
        "\n",
        "    def _analyze_worst_slack_reasons(self, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Analyze reasons for worst slack\"\"\"\n",
        "        worst_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "        second_worst = sorted(all_slack_data, key=lambda x: x['slack'])[1] if len(all_slack_data) > 1 else worst_path\n",
        "\n",
        "        analysis_parts = []\n",
        "\n",
        "        # Analyze the worst path\n",
        "        analysis_parts.append(f\"📊 WORST SLACK ANALYSIS:\")\n",
        "        analysis_parts.append(f\"📍 Worst path: {worst_path['slack']:.3f}ns\")\n",
        "        analysis_parts.append(f\"🔄 From: {worst_path['startpoint']} → {worst_path['endpoint']}\")\n",
        "\n",
        "        # Clock skew analysis\n",
        "        clock_skew = worst_path.get('clock_skew', 'N/A')\n",
        "        if clock_skew != 'N/A':\n",
        "            analysis_parts.append(f\"⏰ Clock skew: {clock_skew:.3f}ns\")\n",
        "            try:\n",
        "                if float(clock_skew) > 0.4:\n",
        "                    analysis_parts.append(\"   ⚠️ HIGH clock skew - impacts timing margin\")\n",
        "                else:\n",
        "                    analysis_parts.append(\"   ✅ Moderate clock skew\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Hold time analysis\n",
        "        hold_time = worst_path.get('hold_time_requirement', 'N/A')\n",
        "        if hold_time != 'N/A':\n",
        "            analysis_parts.append(f\"🔒 Hold time requirement: {hold_time}ns\")\n",
        "            try:\n",
        "                if float(hold_time) > 0.1:\n",
        "                    analysis_parts.append(\"   ⚠️ HIGH hold time requirement\")\n",
        "                else:\n",
        "                    analysis_parts.append(\"   ✅ Normal hold time requirement\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Comparative analysis\n",
        "        analysis_parts.append(f\"\\n📈 COMPARATIVE ANALYSIS:\")\n",
        "        analysis_parts.append(f\"🎯 Slack range: {worst_path['slack']:.3f}ns to {second_worst['slack']:.3f}ns\")\n",
        "        analysis_parts.append(f\"💰 Margin difference: {second_worst['slack'] - worst_path['slack']:.3f}ns\")\n",
        "\n",
        "        # Summary assessment\n",
        "        analysis_parts.append(f\"\\n✅ TIMING STATUS:\")\n",
        "        analysis_parts.append(f\"🎯 Both paths PASS timing (positive slack)\")\n",
        "        analysis_parts.append(f\"⚠️ Small margins indicate timing sensitivity\")\n",
        "        analysis_parts.append(f\"💡 Consider design optimizations for robustness\")\n",
        "\n",
        "        return \"\\n\".join(analysis_parts)\n",
        "\n",
        "    def _generate_slack_improvement_recommendations(self, all_slack_data: List[Dict], question: str) -> str:\n",
        "        \"\"\"Generate comprehensive slack improvement recommendations based on actual data\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available for analysis.\"\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Determine number of recommendations requested\n",
        "        num_points = 10  # Default\n",
        "        if \"5 points\" in question_lower or \"5 point\" in question_lower:\n",
        "                num_points = 5\n",
        "        elif \"3 points\" in question_lower or \"3 point\" in question_lower:\n",
        "            num_points = 3\n",
        "\n",
        "        worst_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "        best_path = max(all_slack_data, key=lambda x: x['slack'])\n",
        "\n",
        "        recommendations = []\n",
        "\n",
        "        # Analyze current timing situation\n",
        "        slack_values = [p['slack'] for p in all_slack_data]\n",
        "        avg_slack = sum(slack_values) / len(slack_values)\n",
        "        min_slack = min(slack_values)\n",
        "\n",
        "        recommendations.append(f\"📊 CURRENT TIMING STATUS:\")\n",
        "        recommendations.append(f\"• Worst slack: {worst_path['slack']:.3f}ns\")\n",
        "        recommendations.append(f\"• Best slack: {best_path['slack']:.3f}ns\")\n",
        "        recommendations.append(f\"• Average slack: {avg_slack:.3f}ns\")\n",
        "        recommendations.append(f\"• All paths PASS timing ✅\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        recommendations.append(f\"🎯 TOP {num_points} SLACK IMPROVEMENT RECOMMENDATIONS:\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        # Clock skew optimization\n",
        "        clock_skews = [p.get('clock_skew', 0) for p in all_slack_data if p.get('clock_skew') != 'N/A']\n",
        "        if clock_skews and max(clock_skews) > 0.4:\n",
        "            recommendations.append(f\"1. 🔧 BALANCE CLOCK SKEW\")\n",
        "            recommendations.append(f\"   Current worst skew: {max(clock_skews):.3f}ns\")\n",
        "            recommendations.append(f\"   → Implement balanced clock distribution\")\n",
        "            recommendations.append(f\"   → Add buffer cells in high-skew regions\")\n",
        "            recommendations.append(f\"   → Optimize clock tree synthesis\")\n",
        "            recommendations.append(\"\")\n",
        "\n",
        "        # Hold time optimization\n",
        "        hold_times = [p.get('hold_time_requirement', 0) for p in all_slack_data if p.get('hold_time_requirement') != 'N/A']\n",
        "        if hold_times and max(hold_times) > 0.1:\n",
        "            recommendations.append(f\"2. ⏰ OPTIMIZE HOLD TIME REQUIREMENTS\")\n",
        "            recommendations.append(f\"   Current worst hold time: {max(hold_times):.3f}ns\")\n",
        "            recommendations.append(f\"   → Add hold buffers in critical paths\")\n",
        "            recommendations.append(f\"   → Optimize flip-flop timing\")\n",
        "            recommendations.append(\"\")\n",
        "\n",
        "        # Design optimization based on path analysis\n",
        "        recommendations.append(f\"{2 if clock_skews and max(clock_skews) > 0.4 else 3 if hold_times and max(hold_times) > 0.1 else 1}. 📐 OPTIMIZE DESIGN TOPOLOGY\")\n",
        "        recommendations.append(f\"   Worst path: {worst_path['startpoint']} → {worst_path['endpoint']}\")\n",
        "        if \"housekeeping\" in worst_path.get('startpoint', ''):\n",
        "            recommendations.append(f\"   → Housekeeping modules often have timing sensitivity\")\n",
        "            recommendations.append(f\"   → Consider register pipelining\")\n",
        "        recommendations.append(f\"   → Review logic synthesis constraints\")\n",
        "        recommendations.append(f\"   → Optimize wire routing and placement\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        # Slack margin improvement\n",
        "        recommendations.append(f\"{3 if clock_skews and max(clock_skews) > 0.4 else 4 if hold_times and max(hold_times) > 0.1 else 2}. 📈 IMPROVE TIMING MARGINS\")\n",
        "        recommendations.append(f\"   Current margins: {min_slack:.3f}ns to {max(slack_values):.3f}ns\")\n",
        "        recommendations.append(f\"   → Target minimum slack > 0.5ns for robustness\")\n",
        "        recommendations.append(f\"   → Consider operating condition guardbands\")\n",
        "        recommendations.append(f\"   → Add timing margin in synthesis\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        # Speed grade optimization\n",
        "        recommendations.append(f\"{4 if clock_skews and max(clock_skews) > 0.4 else 5 if hold_times and max(hold_times) > 0.1 else 3}. 🚀 SPEED GRADE OPTIMIZATION\")\n",
        "        recommendations.append(f\"   → Evaluate slower speed grades for better timing\")\n",
        "        recommendations.append(f\"   → Trade-off performance vs reliability\")\n",
        "        recommendations.append(f\"   → Consider multi-clock domain partitioning\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        # Temperature and voltage optimization\n",
        "        recommendations.append(f\"{5 if clock_skews and max(clock_skews) > 0.4 else 6 if hold_times and max(hold_times) > 0.1 else 4}. 🌡️ TEMPERATURE/VOLTAGE ANALYSIS\")\n",
        "        recommendations.append(f\"   → Analyze timing across temperature corners\")\n",
        "        recommendations.append(f\"   → Consider voltage scaling optimization\")\n",
        "        recommendations.append(f\"   → Review process corner sensitivity\")\n",
        "        recommendations.append(\"\")\n",
        "\n",
        "        # Add remaining recommendations to reach requested count\n",
        "        remaining_count = num_points - (5 if clock_skews and max(clock_skews) > 0.4 else 5 if hold_times and max(hold_times) > 0.1 else 5)\n",
        "\n",
        "        additional_recommendations = [\n",
        "            (\"6. 📋 DESIGN RULE OPTIMIZATION\", \"→ Minimize long wires\\n→ Add repeaters in nets > threshold\\n→ Optimize fanout distribution\"),\n",
        "            (\"7. 🔄 LOGIC OPTIMIZATION\", \"→ Use carry chains efficiently\\n→ Balance combinational logic\\n→ Pipeline critical sections\"),\n",
        "            (\"8. ⚡ POWER OPTIMIZATION\", \"→ Clock gating for unused blocks\\n→ Dynamic voltage scaling\\n→ Reduce switching activity\"),\n",
        "            (\"9. 🎯 CONSTRAINT REFINEMENT\", \"→ Review timing constraints\\n→ Add false/multicycle paths\\n→ Optimize I/O timing\"),\n",
        "            (\"10. 🧪 ANALYSIS IMPROVEMENT\", \"#  → Run Monte Carlo analysis\\n→ Add statistical timing\\n→ Review design coverage\")\n",
        "        ]\n",
        "\n",
        "        for i in range(min(remaining_count, len(additional_recommendations))):\n",
        "            # Calculate proper numbering based on previously added recommendations\n",
        "            base_num = 5\n",
        "            if clock_skews and max(clock_skews) > 0.4:\n",
        "                base_num = 6\n",
        "            elif hold_times and max(hold_times) > 0.1:\n",
        "                base_num = 6\n",
        "            else:\n",
        "                base_num = 4\n",
        "\n",
        "            idx = base_num + i\n",
        "            rec = additional_recommendations[i]\n",
        "            recommendations.append(f\"{idx}. {rec[0]}\")\n",
        "            recommendations.append(f\"   {rec[1]}\")\n",
        "            recommendations.append(\"\")\n",
        "\n",
        "        recommendations.append(\"⚠️  IMPORTANT: Current design PASSES timing. These recommendations optimize margins for robustness.\")\n",
        "\n",
        "        return \"\\n\".join(recommendations)\n",
        "\n",
        "    def _generate_technical_reasoning(self, all_slack_data: List[Dict], question: str) -> str:\n",
        "        \"\"\"Generate technical reasoning about timing relationships\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available for analysis.\"\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Clock skew vs slack analysis\n",
        "        if \"clock skew\" in question_lower or \"skew\" in question_lower:\n",
        "            return self._explain_clock_skew_relationship(all_slack_data, question)\n",
        "\n",
        "        # General timing relationships\n",
        "        return self._explain_timing_relationships(all_slack_data, question)\n",
        "\n",
        "    def _explain_clock_skew_relationship(self, all_slack_data: List[Dict], question: str) -> str:\n",
        "        \"\"\"Explain the relationship between clock skew and slack\"\"\"\n",
        "        worst_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "        best_path = max(all_slack_data, key=lambda x: x['slack'])\n",
        "\n",
        "        # Get actual clock skew values\n",
        "        clock_skews = [p.get('clock_skew', 0) for p in all_slack_data if p.get('clock_skew') != 'N/A']\n",
        "        current_skew = clock_skews[0] if clock_skews else 0.5  # Use first available skew\n",
        "\n",
        "        explanation = []\n",
        "        explanation.append(\"🔬 CLOCK SKEW vs SLACK RELATIONSHIP:\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"📊 CURRENT DATA:\")\n",
        "        explanation.append(f\"• Current clock skew: {current_skew:.3f}ns\")\n",
        "        explanation.append(f\"• Worst slack: {worst_path['slack']:.3f}ns\")\n",
        "        explanation.append(f\"• Best slack: {best_path['slack']:.3f}ns\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"🧠 TECHNICAL EXPLANATION:\")\n",
        "        explanation.append(\"\")\n",
        "        explanation.append(\"Clock skew affects slack through timing paths:\")\n",
        "        explanation.append(\"\")\n",
        "        explanation.append(\"🔸 POSITIVE CLOCK SKEW (launch clock arrives LATER):\")\n",
        "        explanation.append(\"   • Data has MORE time to propagate\")\n",
        "        explanation.append(\"   • → SLACK IMPROVES (becomes more positive)\")\n",
        "        explanation.append(\"   • → Better timing margin\")\n",
        "        explanation.append(\"\")\n",
        "        explanation.append(\"🔸 NEGATIVE CLOCK SKEW (launch clock arrives EARLIER):\")\n",
        "        explanation.append(\"   • Data has LESS time to propagate\")\n",
        "        explanation.append(\"   • → SLACK DEGRADES (becomes less positive)\")\n",
        "        explanation.append(\"   • → Worse timing margin\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"📐 MATHEMATICAL RELATIONSHIP:\")\n",
        "        explanation.append(\"Slack = Required_Time - Arrival_Time\")\n",
        "        explanation.append(\"If clock_skew increases (more positive):\")\n",
        "        explanation.append(\"→ Required_Time increases\")\n",
        "        explanation.append(\"→ Slack = (Required_Time + skew) - Arrival_Time\")\n",
        "        explanation.append(\"→ Slack IMPROVES ✅\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"🎯 ANSWER TO YOUR QUESTION:\")\n",
        "        explanation.append(\"If clock skews become MORE POSITIVE:\")\n",
        "        explanation.append(\"→ SLACKS WILL IMPROVE (become more positive)\")\n",
        "        explanation.append(\"→ Better timing margins\")\n",
        "        explanation.append(\"→ More robust design\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"⚠️  PRACTICAL CONSIDERATIONS:\")\n",
        "        explanation.append(\"• Positive skew helps setup timing\")\n",
        "        explanation.append(\"• But may hurt hold timing\")\n",
        "        explanation.append(\"• Balance between setup and hold is critical\")\n",
        "        explanation.append(\"• Current design shows moderate skew (0.5ns)\")\n",
        "\n",
        "        return \"\\n\".join(explanation)\n",
        "\n",
        "    def _explain_timing_relationships(self, all_slack_data: List[Dict], question: str) -> str:\n",
        "        \"\"\"General timing relationship explanations\"\"\"\n",
        "        explanation = []\n",
        "        explanation.append(\"🔬 TIMING RELATIONSHIPS EXPLANATION:\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        # Basic timing concepts\n",
        "        explanation.append(\"📚 FUNDAMENTAL TIMING CONCEPTS:\")\n",
        "        explanation.append(\"• Setup Time: Data must be stable before clock edge\")\n",
        "        explanation.append(\"• Hold Time: Data must remain stable after clock edge\")\n",
        "        explanation.append(\"• Slack: Margin between arrival and required time\")\n",
        "        explanation.append(\"• Clock Skew: Difference in clock arrival times\")\n",
        "        explanation.append(\"\")\n",
        "\n",
        "        explanation.append(\"🎯 KEY RELATIONSHIPS:\")\n",
        "        explanation.append(\"• More positive slack = Better timing margin\")\n",
        "        explanation.append(\"• Clock skew affects both setup and hold timing\")\n",
        "        explanation.append(\"• Temperature/voltage variations impact timing\")\n",
        "        explanation.append(\"• Process corners affect all timing parameters\")\n",
        "\n",
        "        return \"\\n\".join(explanation)\n",
        "\n",
        "    def _handle_clock_skew_ranking(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle clock skew ranking queries\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available.\"\n",
        "\n",
        "        # Extract number from question\n",
        "        n = self._extract_number_from_question(question)\n",
        "\n",
        "        # Filter paths that have clock skew data\n",
        "        paths_with_skew = []\n",
        "        for path in all_slack_data:\n",
        "            if path.get('clock_skew') != 'N/A':\n",
        "                try:\n",
        "                    clock_skew = float(path['clock_skew'])\n",
        "                    paths_with_skew.append({\n",
        "                        'clock_skew': clock_skew,\n",
        "                        'startpoint': path['startpoint'],\n",
        "                        'endpoint': path['endpoint'],\n",
        "                        'group': path['group'],\n",
        "                        'slack': path['slack']\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        if not paths_with_skew:\n",
        "            return \"No clock skew data found in timing reports\"\n",
        "\n",
        "        # Sort by clock skew (default to worst = highest skew)\n",
        "        question_lower = question.lower()\n",
        "        if \"best\" in question_lower:\n",
        "            sorted_paths = sorted(paths_with_skew, key=lambda x: x['clock_skew'])\n",
        "            direction = \"best\"\n",
        "        else:\n",
        "            sorted_paths = sorted(paths_with_skew, key=lambda x: x['clock_skew'], reverse=True)\n",
        "            direction = \"worst\"\n",
        "\n",
        "        # Get top N paths\n",
        "        top_paths = sorted_paths[:min(n, len(sorted_paths))]\n",
        "\n",
        "        result = f\"Top {len(top_paths)} {direction} clock skew paths:\\n\"\n",
        "        for i, path in enumerate(top_paths, 1):\n",
        "            skew_status = \"⚠️ HIGH\" if path['clock_skew'] > 0.4 else \"📍 NORMAL\"\n",
        "            result += f\"{i}. Clock skew: {path['clock_skew']:.3f}ns [{skew_status}] - Path from {path['startpoint']} to {path['endpoint']} in group {path['group']} (Slack: {path['slack']:.3f}ns)\\n\"\n",
        "\n",
        "        return result.strip()\n",
        "\n",
        "    def _handle_hold_time_ranking(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle hold time ranking queries\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available.\"\n",
        "\n",
        "        # Extract number from question\n",
        "        n = self._extract_number_from_question(question)\n",
        "\n",
        "        # Filter paths that have hold time data\n",
        "        paths_with_hold_time = []\n",
        "        for path in all_slack_data:\n",
        "            if path.get('hold_time_requirement') != 'N/A':\n",
        "                try:\n",
        "                    hold_time = float(path['hold_time_requirement'])\n",
        "                    paths_with_hold_time.append({\n",
        "                        'hold_time_requirement': hold_time,\n",
        "                        'startpoint': path['startpoint'],\n",
        "                        'endpoint': path['endpoint'],\n",
        "                        'group': path['group'],\n",
        "                        'slack': path['slack']\n",
        "                    })\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        if not paths_with_hold_time:\n",
        "            return \"No hold time requirement data found in timing reports\"\n",
        "\n",
        "        # Sort by hold time (default to worst = highest hold time)\n",
        "        question_lower = question.lower()\n",
        "        if \"best\" in question_lower:\n",
        "            sorted_paths = sorted(paths_with_hold_time, key=lambda x: x['hold_time_requirement'])\n",
        "            direction = \"best\"\n",
        "        else:\n",
        "            sorted_paths = sorted(paths_with_hold_time, key=lambda x: x['hold_time_requirement'], reverse=True)\n",
        "            direction = \"worst\"\n",
        "\n",
        "        # Get top N paths\n",
        "        top_paths = sorted_paths[:min(n, len(sorted_paths))]\n",
        "\n",
        "        result = f\"Top {len(top_paths)} {direction} hold time requirement paths:\\n\"\n",
        "        for i, path in enumerate(top_paths, 1):\n",
        "            hold_status = \"⚠️ HIGH\" if path['hold_time_requirement'] > 0.1 else \"📍 NORMAL\"\n",
        "            result += f\"{i}. Hold time: {path['hold_time_requirement']:.3f}ns [{hold_status}] - Path from {path['startpoint']} to {path['endpoint']} in group {path['group']} (Slack: {path['slack']:.3f}ns)\\n\"\n",
        "\n",
        "        return result.strip()\n",
        "\n",
        "    def _handle_counting_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle counting queries with condition parsing\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data available.\"\n",
        "\n",
        "        print(f\"DEBUG: Using direct counting (no LLM)...\")\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Handle simple total counts first\n",
        "        if any(phrase in question_lower for phrase in [\"are there\", \"paths are\", \"how many\", \"total paths\", \"paths total\"]):\n",
        "            # Simple counting - just return total\n",
        "            return f\"Total number of timing paths: {len(all_slack_data)}\"\n",
        "\n",
        "        # Check for conditional counts\n",
        "        if \"slack\" in question_lower:\n",
        "            # Parse conditions like \"slack less than 1ns\", \"slack greater than 1ns\"\n",
        "            if \"less than\" in question_lower or \"below\" in question_lower or \"<\" in question_lower:\n",
        "                # Extract threshold value\n",
        "                threshold = self._extract_threshold_from_question(question, \"less\")\n",
        "                if threshold is not None:\n",
        "                    count = sum(1 for path in all_slack_data if path['slack'] < threshold)\n",
        "                    return f\"Paths with slack less than {threshold}ns: {count} out of {len(all_slack_data)}\"\n",
        "\n",
        "            elif \"greater than\" in question_lower or \"above\" in question_lower or \"more than\" in question_lower or \">\" in question_lower:\n",
        "                # Extract threshold value\n",
        "                threshold = self._extract_threshold_from_question(question, \"greater\")\n",
        "                if threshold is not None:\n",
        "                    count = sum(1 for path in all_slack_data if path['slack'] > threshold)\n",
        "                    return f\"Paths with slack greater than {threshold}ns: {count} out of {len(all_slack_data)}\"\n",
        "\n",
        "            elif \"equal to\" in question_lower or \"==\" in question_lower or \"=\" in question_lower:\n",
        "                # Extract threshold value\n",
        "                threshold = self._extract_threshold_from_question(question, \"equal\")\n",
        "                if threshold is not None:\n",
        "                    count = sum(1 for path in all_slack_data if path['slack'] == threshold)\n",
        "                    return f\"Paths with slack equal to {threshold}ns: {count} out of {len(all_slack_data)}\"\n",
        "\n",
        "            elif \"between\" in question_lower:\n",
        "                # Extract range values\n",
        "                min_val, max_val = self._extract_range_from_question(question)\n",
        "                if min_val is not None and max_val is not None:\n",
        "                    count = sum(1 for path in all_slack_data if min_val <= path['slack'] <= max_val)\n",
        "                    return f\"Paths with slack between {min_val}ns and {max_val}ns: {count} out of {len(all_slack_data)}\"\n",
        "\n",
        "        # Default to total count\n",
        "        total_paths = len(all_slack_data)\n",
        "        return f\"Total number of timing paths: {total_paths}\"\n",
        "\n",
        "    def _handle_statistics_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle statistics queries using fast direct processing\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data available.\"\n",
        "\n",
        "        print(f\"DEBUG: Using direct statistics (no LLM)...\")\n",
        "        slack_values = [path['slack'] for path in all_slack_data]\n",
        "        avg_slack = sum(slack_values) / len(slack_values)\n",
        "        min_slack = min(slack_values)\n",
        "        max_slack = max(slack_values)\n",
        "\n",
        "        return f\"Slack statistics:\\n- Average: {avg_slack:.3f}ns\\n- Minimum: {min_slack:.3f}ns\\n- Maximum: {max_slack:.3f}ns\\n- Total paths: {len(slack_values)}\"\n",
        "\n",
        "    def _handle_filtering_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle filtering queries using fast direct processing\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data available.\"\n",
        "\n",
        "        print(f\"DEBUG: Using direct filtering (no LLM)...\")\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Handle technical reasoning queries\n",
        "        if any(word in question_lower for word in [\"can slacks\", \"if clock\", \"relationship\", \"how does\", \"positive clock\", \"negative clock\", \"skew effect\"]):\n",
        "            return self._generate_technical_reasoning(all_slack_data, question)\n",
        "\n",
        "        # Handle recommendation queries with comprehensive advice\n",
        "        if any(word in question_lower for word in [\"recommend\", \"suggest\", \"optimize\", \"improve\", \"how can we\", \"what can we do\", \"make better\", \"better slack\"]):\n",
        "            return self._generate_slack_improvement_recommendations(all_slack_data, question)\n",
        "\n",
        "        # Check for timing status queries first\n",
        "        if any(word in question_lower for word in [\"failing\", \"violation\", \"fail\", \"failed\", \"passing\", \"pass\", \"critical\", \"borderline\"]):\n",
        "            failing_paths = []\n",
        "            passing_paths = []\n",
        "\n",
        "            for path in all_slack_data:\n",
        "                slack = path['slack']\n",
        "                if slack < 0:\n",
        "                    failing_paths.append(path)\n",
        "                else:\n",
        "                    passing_paths.append(path)\n",
        "\n",
        "            results = []\n",
        "            if any(word in question_lower for word in [\"failing\", \"violation\", \"fail\", \"failed\"]):\n",
        "                if failing_paths:\n",
        "                    results.append(\"🔴 FAILING PATHS (Negative Slack = Timing Violation):\")\n",
        "                    for path in failing_paths:\n",
        "                        results.append(f\"  📍 {path['startpoint']} → {path['endpoint']}: Slack={path['slack']:.3f}ns [VIOLATION]\")\n",
        "                else:\n",
        "                    results.append(\"✅ GOOD NEWS: NO FAILING PATHS!\")\n",
        "                    results.append(\"All paths have positive slack values (timing passes)\")\n",
        "                    results.append(f\"📊 Summary: {len(passing_paths)} paths PASS timing\")\n",
        "\n",
        "            elif any(word in question_lower for word in [\"passing\", \"pass\", \"critica\"]):\n",
        "                critical_count = sum(1 for p in passing_paths if 0 <= p['slack'] < 0.1)\n",
        "                results.append(f\"🟢 TIMING STATUS SUMMARY:\")\n",
        "                results.append(f\"  📈 PASSING PATHS: {len(passing_paths)} paths\")\n",
        "                results.append(f\"  📈 FAILING PATHS: {len(failing_paths)} paths\")\n",
        "                results.append(f\"  ⚠️ CRITICAL PATHS: {critical_count} paths with <0.1ns margin\")\n",
        "\n",
        "            return \"\\n\".join(results) if results else \"No timing status data available\"\n",
        "\n",
        "        # Check if asking for clock skew specifically\n",
        "        if \"clock skew\" in question_lower or \"skew\" in question_lower:\n",
        "            result = \"Clock skew information for all paths:\\n\"\n",
        "            for i, path in enumerate(all_slack_data, 1):\n",
        "                clock_skew = path.get('clock_skew', 'N/A')\n",
        "                if clock_skew != 'N/A':\n",
        "                    result += f\"{i}. Path {path['startpoint']} to {path['endpoint']} - Clock skew: {clock_skew}ns - Slack: {path['slack']}ns\\n\"\n",
        "                else:\n",
        "                    result += f\"{i}. Path {path['startpoint']} to {path['endpoint']} - Clock skew: Not available - Slack: {path['slack']}ns\\n\"\n",
        "            return result.strip()\n",
        "\n",
        "        # Check if asking for hold time requirements specifically\n",
        "        elif \"hold time\" in question_lower:\n",
        "            result = \"Hold time requirements for all paths:\\n\"\n",
        "            for i, path in enumerate(all_slack_data, 1):\n",
        "                hold_time = path.get('hold_time_requirement', 'N/A')\n",
        "                if hold_time != 'N/A':\n",
        "                    result += f\"{i}. Path {path['startpoint']} to {path['endpoint']} - Hold time requirement: {hold_time}ns - Slack: {path['slack']}ns\\n\"\n",
        "                else:\n",
        "                    result += f\"{i}. Path {path['startpoint']} to {path['endpoint']} - Hold time requirement: Not available - Slack: {path['slack']}ns\\n\"\n",
        "            return result.strip()\n",
        "\n",
        "        # Check if asking for startpoints/endpoints specifically\n",
        "        elif \"startpoint\" in question_lower and \"endpoint\" in question_lower:\n",
        "            result = \"Startpoints and endpoints for all paths:\\n\"\n",
        "            for i, path in enumerate(all_slack_data, 1):\n",
        "                result += f\"{i}. Startpoint: {path['startpoint']} - Endpoint: {path['endpoint']} - Slack: {path['slack']}ns\\n\"\n",
        "            return result.strip()\n",
        "\n",
        "        else:\n",
        "            # Generic filtering - show all paths with all details\n",
        "            result = \"All timing paths with details:\\n\"\n",
        "            for i, path in enumerate(all_slack_data, 1):\n",
        "                result += f\"{i}. Slack: {path['slack']}ns - Start: {path['startpoint']} - End: {path['endpoint']} - Group: {path['group']}\\n\"\n",
        "                if path.get('clock_skew') != 'N/A':\n",
        "                    result += f\"   Clock skew: {path['clock_skew']}ns\\n\"\n",
        "                if path.get('hold_time_requirement') != 'N/A':\n",
        "                    result += f\"   Hold time requirement: {path['hold_time_requirement']}ns\\n\"\n",
        "            return result.strip()\n",
        "\n",
        "    def _handle_navigation_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle navigation queries (next path, etc.)\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No slack data available.\"\n",
        "\n",
        "        # Sort paths by slack (worst to best)\n",
        "        sorted_paths = sorted(all_slack_data, key=lambda x: x['slack'])\n",
        "\n",
        "        # Get the next path after current index\n",
        "        if self.current_path_index < len(sorted_paths):\n",
        "            next_path = sorted_paths[self.current_path_index]\n",
        "            self.current_path_index += 1\n",
        "            return f\"Next path slack: {next_path['slack']}ns for the path from {next_path['startpoint']} to {next_path['endpoint']} in group {next_path['group']}.\"\n",
        "        else:\n",
        "            return \"No more paths available. All paths have been shown.\"\n",
        "\n",
        "    def _extract_number_from_question(self, question: str) -> int:\n",
        "        \"\"\"Extract number from question (e.g., 'top 2 worst' -> 2)\"\"\"\n",
        "        import re\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Look for patterns like \"top 2\", \"worst 3\", \"2 worst\", etc.\n",
        "        patterns = [\n",
        "            r'top\\s+(\\d+)',\n",
        "            r'worst\\s+(\\d+)',\n",
        "            r'(\\d+)\\s+worst',\n",
        "            r'(\\d+)\\s+best',\n",
        "            r'best\\s+(\\d+)',\n",
        "            r'(\\d+)\\s+paths?'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, question_lower)\n",
        "            if match:\n",
        "                return int(match.group(1))\n",
        "\n",
        "        # Default to 2 if no number found\n",
        "        return 2\n",
        "\n",
        "    def _extract_threshold_from_question(self, question: str, condition_type: str) -> float:\n",
        "        \"\"\"Extract numerical threshold from questions like 'slack less than 1ns'\"\"\"\n",
        "        import re\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Look for patterns with numbers followed by 'ns'\n",
        "        patterns = [\n",
        "            r'(\\d+\\.?\\d*)\\s*ns',  # \"1ns\", \"1.5ns\", etc.\n",
        "            r'(\\d+\\.?\\d*)',       # Just numbers\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, question_lower)\n",
        "            if matches:\n",
        "                try:\n",
        "                    value = float(matches[-1])  # Take the last number found\n",
        "                    return value\n",
        "                except (ValueError, IndexError):\n",
        "                    continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_range_from_question(self, question: str) -> tuple:\n",
        "        \"\"\"Extract range values from questions like 'slack between 0.5ns and 1ns'\"\"\"\n",
        "        import re\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Look for \"between X and Y\" patterns\n",
        "        pattern = r'between\\s+(\\d+\\.?\\d*)\\s+ns?\\s+and\\s+(\\d+\\.?\\d*)\\s+ns?'\n",
        "        match = re.search(pattern, question_lower)\n",
        "        if match:\n",
        "            try:\n",
        "                min_val = float(match.group(1))\n",
        "                max_val = float(match.group(2))\n",
        "                return min_val, max_val\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "\n",
        "        # Fallback: extract all numbers\n",
        "        numbers = re.findall(r'(\\d+\\.?\\d*)', question_lower)\n",
        "        if len(numbers) >= 2:\n",
        "            try:\n",
        "                min_val = float(numbers[0])\n",
        "                max_val = float(numbers[1])\n",
        "                return min_val, max_val\n",
        "            except (ValueError, IndexError):\n",
        "                pass\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    def _create_focused_context(self, all_slack_data: List[Dict], question: str) -> str:\n",
        "        \"\"\"Create focused context to avoid overwhelming LLM\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available.\"\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Focus on relevant data based on question\n",
        "        if \"worst\" in question_lower or \"bad\" in question_lower:\n",
        "            worst_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "            return f\"Worst path: {worst_path['slack']}ns slack, {worst_path['startpoint']}→{worst_path['endpoint']}, clock_skew:{worst_path.get('clock_skew', 'N/A')}, hold_time:{worst_path.get('hold_time_requirement', 'N/A')}\"\n",
        "\n",
        "        elif \"best\" in question_lower or \"good\" in question_lower:\n",
        "            best_path = max(all_slack_data, key=lambda x: x['slack'])\n",
        "            return f\"Best path: {best_path['slack']}ns slack, {best_path['startpoint']}→{best_path['endpoint']}\"\n",
        "\n",
        "        else:\n",
        "            # General summary\n",
        "            slack_values = [p['slack'] for p in all_slack_data]\n",
        "            return f\"{len(all_slack_data)} paths: slack range {min(slack_values):.3f} to {max(slack_values):.3f}ns, all positive (pass timing)\"\n",
        "\n",
        "    def _generate_llm_response(self, question: str, context: str) -> str:\n",
        "        \"\"\"Generate response using CodeLlama with proper attention mask and timeout\"\"\"\n",
        "        if not self.llm_model or not self.tokenizer:\n",
        "            return \"LLM not available for response generation.\"\n",
        "\n",
        "        # Create focused context to avoid overwhelming the model\n",
        "        summary_context = self._create_focused_context(self._get_all_slack_data(), question)\n",
        "\n",
        "        prompt = f\"\"\"<s>[INST] Timing Analysis Question:\n",
        "\n",
        "Data: {summary_context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer briefly (2-4 lines): [/INST]\"\"\"\n",
        "\n",
        "        try:\n",
        "            print(\"DEBUG: Tokenizing prompt...\")\n",
        "            inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048)\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            print(f\"DEBUG: Input shape: {inputs.shape}\")\n",
        "\n",
        "            # Create attention mask\n",
        "            attention_mask = (inputs != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "            print(\"DEBUG: Starting LLM generation...\")\n",
        "            import threading\n",
        "            import time\n",
        "\n",
        "            result = [None]\n",
        "            exception = [None]\n",
        "\n",
        "            def generate_worker():\n",
        "                try:\n",
        "                    with torch.no_grad():\n",
        "                        outputs = self.llm_model.generate(\n",
        "                            inputs,\n",
        "                            attention_mask=attention_mask,\n",
        "                            max_new_tokens=50,  # Reduced for faster generation\n",
        "                            num_return_sequences=1,\n",
        "                            temperature=0.3,  # Lower temp for consistency\n",
        "                            do_sample=False,   # Deterministic\n",
        "                            pad_token_id=self.tokenizer.eos_token_id,\n",
        "                            eos_token_id=self.tokenizer.eos_token_id,\n",
        "                            early_stopping=True\n",
        "                        )\n",
        "                        result[0] = outputs\n",
        "                except Exception as e:\n",
        "                    exception[0] = e\n",
        "\n",
        "            # Start generation in a thread\n",
        "            thread = threading.Thread(target=generate_worker)\n",
        "            thread.start()\n",
        "            thread.join(timeout=15)  # 15 second timeout\n",
        "\n",
        "            if thread.is_alive():\n",
        "                print(\"DEBUG: LLM generation timed out after 15s, killing thread and using fallback...\")\n",
        "                # Thread is still alive, can't kill it cleanly, but let it continue in background\n",
        "                return self._generate_fallback_answer(question, None)\n",
        "\n",
        "            if exception[0]:\n",
        "                print(f\"DEBUG: LLM generation failed: {exception[0]}, using fallback...\")\n",
        "                return self._generate_fallback_answer(question, None)\n",
        "\n",
        "            if result[0] is None:\n",
        "                print(\"DEBUG: No result from LLM, using fallback...\")\n",
        "                return self._generate_fallback_answer(question, None)\n",
        "\n",
        "            outputs = result[0]\n",
        "\n",
        "            print(\"DEBUG: LLM generation completed, decoding...\")\n",
        "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            answer = response.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "            print(f\"DEBUG: Response decoded, length: {len(answer)}\")\n",
        "\n",
        "            # If answer is too short or seems incomplete, use fallback\n",
        "            if len(answer.strip()) < 10:\n",
        "                print(\"DEBUG: LLM response too short, using fallback...\")\n",
        "                return self._generate_fallback_answer(question, None)\n",
        "\n",
        "            return answer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in LLM response generation: {e}\")\n",
        "            return self._generate_fallback_answer(question, None)\n",
        "\n",
        "    def _generate_fallback_answer(self, question: str, context_or_data) -> str:\n",
        "        \"\"\"Generate fallback answer when LLM fails\"\"\"\n",
        "        print(\"DEBUG: Generating fallback answer...\")\n",
        "\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Always get fresh data for fallback\n",
        "        all_slack_data = self._get_all_slack_data()\n",
        "\n",
        "        # Handle specific question types with direct logic\n",
        "        if \"worst slack\" in question_lower or \"reason\" in question_lower:\n",
        "            if all_slack_data:\n",
        "                worst_path = min(all_slack_data, key=lambda x: x['slack'])\n",
        "                clock_skew = worst_path.get('clock_skew', 'N/A')\n",
        "                hold_req = worst_path.get('hold_time_requirement', 'N/A')\n",
        "\n",
        "                reason_parts = []\n",
        "                if clock_skew != 'N/A':\n",
        "                    try:\n",
        "                        if float(clock_skew) > 0.4:\n",
        "                            reason_parts.append(f\"High clock skew ({clock_skew}ns)\")\n",
        "                    except:\n",
        "                        pass\n",
        "                if hold_req != 'N/A':\n",
        "                    try:\n",
        "                        if float(hold_req) > 0.1:\n",
        "                            reason_parts.append(f\"High hold time requirement ({hold_req}ns)\")\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                if not reason_parts:\n",
        "                    reason_parts.append(\"Slack is positive but small margin\")\n",
        "\n",
        "                return f\"\"\"Worst slack: {worst_path['slack']:.3f}ns for {worst_path['startpoint']} → {worst_path['endpoint']}\n",
        "\n",
        "Potential reasons:\n",
        "{chr(10).join('- ' + reason for reason in reason_parts)}\n",
        "\n",
        "Note: This path still passes timing (positive slack) but has the smallest margin.\"\"\"\n",
        "\n",
        "        elif \"reason\" in question_lower or \"why\" in question_lower:\n",
        "            if all_slack_data:\n",
        "                return f\"\"\"Timing Analysis Summary:\n",
        "- Total paths: {len(all_slack_data)}\n",
        "- All paths PASS timing (positive slack)\n",
        "- Slack range: {min(p['slack'] for p in all_slack_data):.3f}ns to {max(p['slack'] for p in all_slack_data):.3f}ns\n",
        "\n",
        "Potential timing concerns:\n",
        "- Small slack margins (both < 1ns)\n",
        "- Clock skew and hold time requirements may impact design margin\"\"\"\n",
        "\n",
        "        else:\n",
        "            return f\"Analysis unavailable due to LLM timeout. Raw data: {len(all_slack_data)} paths processed.\"\n",
        "\n",
        "    def query(self, question: str, top_k: int = 3) -> str:\n",
        "        \"\"\"Query the RAG system\"\"\"\n",
        "        import time\n",
        "        query_start = time.time()\n",
        "\n",
        "        print(f\"DEBUG: Query method called with: '{question}'\")\n",
        "\n",
        "        try:\n",
        "            # Add to history\n",
        "            print(\"DEBUG: Adding to history...\")\n",
        "            self.history.append({'question': question, 'answer': ''})\n",
        "\n",
        "            # Get all slack data for comprehensive analysis\n",
        "            print(\"DEBUG: Getting all slack data...\")\n",
        "            all_slack_data = self._get_all_slack_data()\n",
        "            print(f\"DEBUG: Retrieved {len(all_slack_data)} slack data entries\")\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Error in initial query setup: {e}\")\n",
        "            return f\"Error in query setup: {e}\"\n",
        "\n",
        "        # Classify the question\n",
        "        classify_start = time.time()\n",
        "        print(\"DEBUG: Classifying question...\")\n",
        "        question_type = self._classify_question(question, all_slack_data)\n",
        "        classify_time = time.time() - classify_start\n",
        "        print(f\"DEBUG: Question classified as: '{question_type}' (took {classify_time:.2f}s)\")\n",
        "\n",
        "        # Generate response based on high-level question type\n",
        "        handler_start = time.time()\n",
        "        print(f\"DEBUG: Routing to handler for type: {question_type}\")\n",
        "\n",
        "        if question_type == \"complex\":\n",
        "            print(\"DEBUG: Calling complex query handler\")\n",
        "            result = self._handle_complex_query(question, all_slack_data)\n",
        "        elif question_type == \"navigation\":\n",
        "            print(\"DEBUG: Calling navigation handler\")\n",
        "            result = self._handle_navigation_query(question, all_slack_data)\n",
        "        elif question_type == \"ranking\":\n",
        "            print(\"DEBUG: Calling ranking handler\")\n",
        "            result = self._handle_ranking_query(question, all_slack_data)\n",
        "        elif question_type == \"counting\":\n",
        "            print(\"DEBUG: Calling counting handler\")\n",
        "            result = self._handle_counting_query(question, all_slack_data)\n",
        "        elif question_type == \"statistics\":\n",
        "            print(\"DEBUG: Calling statistics handler\")\n",
        "            result = self._handle_statistics_query(question, all_slack_data)\n",
        "        elif question_type == \"filtering\":\n",
        "            print(\"DEBUG: Calling filtering handler\")\n",
        "            result = self._handle_filtering_query(question, all_slack_data)\n",
        "\n",
        "        # Add final timing\n",
        "        handler_end = time.time()\n",
        "        query_end = time.time()\n",
        "        handler_time = handler_end - handler_start\n",
        "        total_time = query_end - query_start\n",
        "        print(f\"DEBUG: Handler completed in {handler_time:.2f}s, total query time: {total_time:.2f}s\")\n",
        "\n",
        "        # Add else clause for unmatched types\n",
        "        if question_type not in [\"ranking\", \"counting\", \"statistics\", \"filtering\", \"navigation\", \"complex\"]:\n",
        "            result = f\"I don't understand the question type. Please ask about rankings (worst/best paths), counts (how many paths), statistics (slack analysis), or filtering (show paths).\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _handle_complex_query(self, question: str, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Handle complex queries using LLM with structured context\"\"\"\n",
        "        if not all_slack_data:\n",
        "            return \"No timing data available for analysis.\"\n",
        "\n",
        "        print(\"DEBUG: Using LLM for complex query analysis...\")\n",
        "\n",
        "        # Prepare comprehensive data context for LLM\n",
        "        data_context = self._prepare_comprehensive_context(all_slack_data)\n",
        "\n",
        "        # Use the LLM for complex analysis\n",
        "        return self._generate_llm_response(question, data_context)\n",
        "\n",
        "    def _prepare_comprehensive_context(self, all_slack_data: List[Dict]) -> str:\n",
        "        \"\"\"Prepare comprehensive timing data context for LLM analysis\"\"\"\n",
        "        context = \"TIMING ANALYSIS DATA:\\n\"\n",
        "        context += f\"Total paths analyzed: {len(all_slack_data)}\\n\\n\"\n",
        "\n",
        "        # Add summary statistics\n",
        "        slack_values = [path['slack'] for path in all_slack_data]\n",
        "        context += f\"SLACK SUMMARY:\\n\"\n",
        "        context += f\"- Minimum slack: {min(slack_values):.3f}ns\\n\"\n",
        "        context += f\"- Maximum slack: {max(slack_values):.3f}ns\\n\"\n",
        "        context += f\"- Average slack: {sum(slack_values)/len(slack_values):.3f}ns\\n\\n\"\n",
        "\n",
        "        # Add individual path details\n",
        "        context += f\"DETAILED PATH ANALYSIS:\\n\"\n",
        "        for i, path in enumerate(all_slack_data, 1):\n",
        "            context += f\"Path {i}:\\n\"\n",
        "            context += f\"  Startpoint: {path['startpoint']}\\n\"\n",
        "            context += f\"  Endpoint: {path['endpoint']}\\n\"\n",
        "            context += f\"  Slack: {path['slack']}ns\\n\"\n",
        "            context += f\"  Group: {path['group']}\\n\"\n",
        "\n",
        "            # Add clock skew if available\n",
        "            if path.get('clock_skew'):\n",
        "                clock_skew = path['clock_skew']\n",
        "                context += f\"  Clock skew: {clock_skew}ns\\n\"\n",
        "\n",
        "                # Add interpretation\n",
        "                current_slack = slack_values[i-1]\n",
        "                if current_slack < 0:\n",
        "                    context += f\"  Slack analysis: TIMING FAILURE (negative slack = violation)\\n\"\n",
        "                elif current_slack < 0.1:\n",
        "                    context += f\"  Slack analysis: CRITICAL (very small positive margin)\\n\"\n",
        "                else:\n",
        "                    context += f\"  Slack analysis: TIMING PASS (adequate positive margin)\\n\"\n",
        "\n",
        "                if clock_skew > 0.4:\n",
        "                    context += f\"  Clock skew analysis: HIGH (may impact timing margin)\\n\"\n",
        "\n",
        "            # Add hold time if available\n",
        "            if path.get('hold_time_requirement'):\n",
        "                hold_time = path['hold_time_requirement']\n",
        "                context += f\"  Hold time requirement: {hold_time}ns\\n\"\n",
        "\n",
        "            context += \"\\n\"\n",
        "\n",
        "        # Add analysis guidance\n",
        "        context += \"ANALYSIS GUIDANCE:\\n\"\n",
        "        context += \"- POSITIVE slack = timing PASS (data arrives before required time)\\n\"\n",
        "        context += \"- NEGATIVE slack = timing FAILURE (violation)\\n\"\n",
        "        context += \"- Slack < 0.1ns = critical margin (close to violation)\\n\"\n",
        "        context += \"- Clock skew > 0.4ns typically impacts timing margin\\n\"\n",
        "        context += \"- Current data shows ONLY positive slacks (all paths PASS)\\n\"\n",
        "\n",
        "        return context\n",
        "\n",
        "def main():\n",
        "    # Create RAG system\n",
        "    rag = ImprovedLocalTimingRAG()\n",
        "\n",
        "    # Index timing reports\n",
        "    rag.index_timing_reports('./timing_reports/')\n",
        "\n",
        "    # Interactive query loop\n",
        "    print(\"\\n=== Improved Timing RAG System Ready ===\")\n",
        "    print(\"Ask questions about your timing data. Type 'quit' to exit.\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nQuestion: \").strip()\n",
        "        if question.lower() in ['quit', 'exit', 'q']:\n",
        "            break\n",
        "\n",
        "        if question:\n",
        "            answer = rag.query(question)\n",
        "            print(f\"Answer: {answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939,
          "referenced_widgets": [
            "b6a732a84a2a40c9a086895c919929ae",
            "275ccb31698c4106a81f354a171b8f49",
            "522c65bbe68b47d1886f7b81295e8a62",
            "3f58f55d70b3412da1ff3f473809a924",
            "78fdc131b2934a749791ff0f0fc91494",
            "7ca72009cc0b45b1bbabfffe8cc0d62e",
            "38bd5ddd165c4446b24654fbc7c64df8",
            "9fefe47d268045b78224582ce3031aa2",
            "77e4a2a8bc9f403092ea56dd74dfd012",
            "22b0cc4ae1c0457ea3e155ebb377b5a8",
            "b81f730ec7a74ba4835444377ef9f200"
          ]
        },
        "id": "_C81PAYF-pZi",
        "outputId": "d20ee850-318d-477c-f717-fe65c187f2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading codellama/CodeLlama-7b-Instruct-hf...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6a732a84a2a40c9a086895c919929ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded codellama/CodeLlama-7b-Instruct-hf\n",
            "Using device: cuda\n",
            "Using in-memory database\n",
            "=== Indexing timing reports ===\n",
            "Found 1 JSON files to index\n",
            "Indexing caravel.min-hkspi_clk-min_timing_full.json...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully indexed caravel.min-hkspi_clk-min_timing_full.json with 659 paths\n",
            "Indexing complete!\n",
            "\n",
            "=== Improved Timing RAG System Ready ===\n",
            "Ask questions about your timing data. Type 'quit' to exit.\n",
            "\n",
            "Question: what are top 2 worst paths\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Query method called with: 'what are top 2 worst paths'\n",
            "DEBUG: Adding to history...\n",
            "DEBUG: Getting all slack data...\n",
            "DEBUG: Getting all slack data from collection...\n",
            "DEBUG: Calling collection.get()...\n",
            "DEBUG: Collection.get() returned 659 entries\n",
            "DEBUG: Processed 659 slack data entries\n",
            "DEBUG: Retrieved 659 slack data entries\n",
            "DEBUG: Classifying question...\n",
            "DEBUG: Classifying question: 'what are top 2 worst paths'\n",
            "DEBUG: Query complexity: simple\n",
            "DEBUG: Using fast pattern matching for simple query...\n",
            "DEBUG: Pattern matched as 'ranking'\n",
            "DEBUG: Question classified as: 'ranking' (took 0.00s)\n",
            "DEBUG: Routing to handler for type: ranking\n",
            "DEBUG: Calling ranking handler\n",
            "DEBUG: Handling ranking query: what are top 2 worst paths\n",
            "DEBUG: Found 659 paths\n",
            "DEBUG: Extracted number: 2\n",
            "DEBUG: Using direct processing (no LLM - it's too slow!)\n",
            "DEBUG: Handler completed in 0.00s, total query time: 0.05s\n",
            "Answer: Top 2 worst slack paths:\n",
            "1. Slack: 0.3252ns - Path from chip_core/housekeeping/_6778_ to chip_core/housekeeping/_6778_ in group hkspi_clk\n",
            "2. Slack: 0.6112ns - Path from chip_core/housekeeping/_6656_ to chip_core/housekeeping/_6654_ in group hkspi_clk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "WbGqVECB2BFC",
        "outputId": "496a3ac1-bbf9-45d8-f374-07008ce56b4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'RAG_STA_reports_codellama1.ipynb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1227708613.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Clean and download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mclean_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RAG_STA_reports_codellama1.ipynb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RAG_STA_reports_codellama1.ipynb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1227708613.py\u001b[0m in \u001b[0;36mclean_notebook\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Clean widget metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'RAG_STA_reports_codellama1.ipynb'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPCh3b3A6sW3cfpTM0AgnNk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b6a732a84a2a40c9a086895c919929ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_275ccb31698c4106a81f354a171b8f49",
              "IPY_MODEL_522c65bbe68b47d1886f7b81295e8a62",
              "IPY_MODEL_3f58f55d70b3412da1ff3f473809a924"
            ],
            "layout": "IPY_MODEL_78fdc131b2934a749791ff0f0fc91494"
          }
        },
        "275ccb31698c4106a81f354a171b8f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ca72009cc0b45b1bbabfffe8cc0d62e",
            "placeholder": "​",
            "style": "IPY_MODEL_38bd5ddd165c4446b24654fbc7c64df8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "522c65bbe68b47d1886f7b81295e8a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fefe47d268045b78224582ce3031aa2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77e4a2a8bc9f403092ea56dd74dfd012",
            "value": 2
          }
        },
        "3f58f55d70b3412da1ff3f473809a924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22b0cc4ae1c0457ea3e155ebb377b5a8",
            "placeholder": "​",
            "style": "IPY_MODEL_b81f730ec7a74ba4835444377ef9f200",
            "value": " 2/2 [01:16&lt;00:00, 34.88s/it]"
          }
        },
        "78fdc131b2934a749791ff0f0fc91494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ca72009cc0b45b1bbabfffe8cc0d62e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38bd5ddd165c4446b24654fbc7c64df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fefe47d268045b78224582ce3031aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77e4a2a8bc9f403092ea56dd74dfd012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22b0cc4ae1c0457ea3e155ebb377b5a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b81f730ec7a74ba4835444377ef9f200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}